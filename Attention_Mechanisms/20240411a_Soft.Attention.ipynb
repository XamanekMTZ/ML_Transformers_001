{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Attention Mechanism (Bahdanau et al., 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Attention Network} = \\text{softmax}(V(\\tanh(W_1(a)+ W_2(h_0))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta ecuación describe un mecanismo de atención, el cual es un componente crucial en muchas arquitecturas de redes neuronales en el procesamiento de lenguaje natural (NLP).  \n",
    "  \n",
    "En este notebook, se implementa un mecanismo de atención suave en PyTorch y utilizando solo Numpy para la implementación de este mecanismo/formula.  \n",
    "  \n",
    "Los componentes de la ecuación son:\n",
    "\n",
    "- $a$: es la secuencia de entrada, la cual es una secuencia de vectores de características, tipicamente se obtiene de una capa de encoder en un modelo de secuencia a secuencia.En el contexto de un modelo RNN, $a$ es la secuencia de estados ocultos de la capa de encoder correspondientes a cada token en la secuencia de entrada.\n",
    "\n",
    "- $h0$: simboliza el estado inicial oculto del decoder. en el proceso iterativo de decodificación, $h0$ es el estado oculto del decoder en el paso de tiempo anterior.\n",
    "\n",
    "- $W1$ y $W2$: son matrices de peso entrenables. $W1$ es aplicado a las entradas $a$ y $W2$ es aplicado al estado oculto $h0$. Estas matrices transforman sus respectivas entradas en un espacio en común donde pueden ser combinadas para calcular la atención.\n",
    "\n",
    "- $V$: Otro vector de peso entrenable que es aplicado despues de la adición y transformación no lineal de las anotaciones del encoder y el estado oculto del decoder. Este vector proyecta la combinación en un espacio de atención, donde se puede interpretar como un score de atención no normalizado.\n",
    "\n",
    "- $\\tanh$: es la función de activación tangente hiperbólica. Es una función de activación no lineal usada para introducir la no linealidad en la transformación de las entradas, permitiendo al modelo aprender relaciones complejas.\n",
    "\n",
    "- $\\text{softmax}$: es una función de activación que toma un vector de entrada y lo normaliza en un vector de probabilidades. En el contexto de la atención, el vector de entrada es el score de atención no normalizado, y el vector de salida es el score de atención normalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad y proposito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mecanismo de atención le permite al modelo enfocarse en diferentes partes de la secuencia de entrada en cada paso de la secuencia de salida, habilitando más generación de salidas consiente del contexto.  \n",
    "  \n",
    "Selecciona dinamicamente que partes de las anotaciones de la entrada son más relevantes para predecir cada token de salida, solucionando la limitante de los vectores de salida de longitud fija en los modelos de secuencia a secuencia tradicionales.  \n",
    "  \n",
    "La ecuación calcula el set de pesos de atención, que son usados para producir una suma ponderada de las anotaciones de entrada. Esta suma ponderada se convierte en el vector de contexto para la salida en el paso de tiempo actual, proporcionando entradas personalizadas al decoder basado en que necesita el modelo para predecir lo siguiente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La operación $W1(a)$ y $W2(h0)$ transforman las anotaciones de entrada y el estado oculto inicial del decoder en un nuevo espacio donde pueden ser comparados. El objetivo es entender que tan relevante es cada parte de la entrada con respecto del estado actual del decoder.  \n",
    "  \n",
    "- La adición de estas transformaciones seguidos de la no linearidad $\\tanh$ combina estas dos fuentes de información en una representación única que captura ambos contenidos de entrada y el actual enfoque del decoder.  \n",
    "  \n",
    "- La operación $V$ y la función de activación $\\text{softmax}$ combierten esta representación combinada en un set de pesos de atención. Estos pesos determinan que tanto debería contribuir cada parte de la entrada al paso actual del decoder.  \n",
    "  \n",
    "- El mecanismo de atención mejora la habilidad del modelo de capturar dependencias a larga distancia y administrar entradas y salidas de diferente longitud, solucionando retos clave en tareas de modelado de secuencias.  \n",
    "  \n",
    "Esta ecuación es un componente fundamental en la implementación del mecanismo de atención en los modelos de redes neuronales, especialmente en el contexto de NLP y tareas de modelado de secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A continuación un ejemplo de implementación de este mecanismo en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class AttentionNetwork(nn.Module):\n",
    "  def __init__( self, annotation_dim, hidden_dim, attention_dim ):\n",
    "    \"\"\"\n",
    "    Inicializar la red de atención.\n",
    "\n",
    "    Parametros:\n",
    "    - annotation_dim: Dimensión de las anotaciones.\n",
    "    - hidden_dim: Dimensión de la capa oculta.\n",
    "    - attention_dim: Dimensión de la capa de atención.\n",
    "    \"\"\"\n",
    "    super( AttentionNetwork, self).__init__()\n",
    "\n",
    "    # Definir las matrices de pesos para la red de atención.\n",
    "    self.W1 = nn.Linear( annotation_dim, attention_dim, bias = False )\n",
    "    # Aplica W1 a las anotaciones.\n",
    "    self.W2 = nn.Linear( hidden_dim, attention_dim, bias = False )\n",
    "    # Aplica W2 a la capa oculta.\n",
    "    self.V = nn.Linear( attention_dim, 1, bias = False )\n",
    "    # Aplica V a la salida de tanh de la suma de W1 y W2.\n",
    "\n",
    "  def forward( self, annotations, hidden ):\n",
    "    \"\"\"\n",
    "    Forward pass de la red de atención. (Esto solo quiere decir que se aplican las operaciones definidas en el constructor).\n",
    "\n",
    "    Parametros:\n",
    "    - annotations: Tensor conteniendo las anotaciones provenientes del encoder ( shape: batch_size x seq_len x annotation_dim ).\n",
    "    - hidden: Tensor conteniendo la capa oculta actual del decoder ( shape: batch_size x hidden_dim ).\n",
    "\n",
    "    Retorna:\n",
    "    - attention_weights: Tensor conteniendo los pesos de atención para cada anotación ( shape: batch_size x seq_len ).\n",
    "    \"\"\"\n",
    "\n",
    "    # Espandir la capa oculta para que tenga la misma forma que las anotaciones para poder sumarlas.\n",
    "    hidden = hidden.unsqueeze( 1 ).expand_as( annotations )\n",
    "\n",
    "    # Calcular los pesos de atención.\n",
    "    attn_scores = self.V( torch.tanh( self.W1( annotations ) + self.W2( hidden ) ) )\n",
    "\n",
    "    # oprimir la ultima dimensión y aplicar softmax para obtener los pesos de atención.\n",
    "    attention_weights = F.softmax( attn_scores.squeeze( -1 ), dim = -1 )\n",
    "\n",
    "    return attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La forma de los pesos de atencion: torch.Size([2, 10])\n",
      "tensor([[0.1254, 0.1272, 0.1006, 0.1343, 0.0925, 0.1244, 0.0942, 0.0652, 0.0572,\n",
      "         0.0791],\n",
      "        [0.1020, 0.1035, 0.1168, 0.0840, 0.1316, 0.0810, 0.1046, 0.0723, 0.0937,\n",
      "         0.1105]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Asumimos que ya se ha definido el encoder y el decoder.\n",
    "# y que estas son las dimensiones de las anotaciones y la capa oculta.\n",
    "annotation_dim = 256 # Dimensión de las anotaciones de salida del encoder\n",
    "hidden_dim = 256 # Dimensión de la capa oculta del decoder\n",
    "attention_dim = 256 # Dimensión de representación intermedia de la red de atención\n",
    "\n",
    "# Inicializar la red de atención.\n",
    "attention_network = AttentionNetwork( annotation_dim, hidden_dim, attention_dim )\n",
    "\n",
    "# Ejemplo de anotaciones y capa oculta para probar la red de atención.\n",
    "batch_size = 2 # El valor debe de \n",
    "seq_len = 10\n",
    "annotations = torch.randn( batch_size, seq_len, annotation_dim )\n",
    "hidden = torch.randn( batch_size, hidden_dim )\n",
    "\n",
    "# Calcular los pesos de atención.\n",
    "attention_weights = attention_network( annotations, hidden )\n",
    "\n",
    "print( f\"La forma de los pesos de atencion: {attention_weights.shape}\")\n",
    "print( attention_weights )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A continuación un ejemplo de implementación de este mecanismo en Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\" Cálcula los valores softmax por cada set de Scores en x\"\"\"\n",
    "  e_x = np.exp( x - np.max( x, axis = -1, keepdims = True ) )\n",
    "  return e_x / e_x.sum( axis = -1, keepdims = True )\n",
    "\n",
    "class AttentionNetworkNumpy:\n",
    "  def __init__( self, annotation_dim, hidden_dim, attention_dim ):\n",
    "    \"\"\"\n",
    "    Inicializar la red de atención con arreglos Numpy\n",
    "\n",
    "    Parametros:\n",
    "    - annotation_dim: Dimensión de las entradas de anotaciones (a)\n",
    "    - hidden_dim: Dimensión de los estados ocultos del decoder (h_0)\n",
    "    - attention_dim: Dimensión intermedia, representación del mecanismo de atención\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar los pesos con valores aleatorios (semi aleatorios)\n",
    "    self.W1 = np.random.rand( annotation_dim, attention_dim )\n",
    "    self.W2 = np.random.rand( hidden_dim, attention_dim )\n",
    "    self.V = np.random.rand( attention_dim, 1 )\n",
    "\n",
    "  def forward( self, annotations, hidden ):\n",
    "    \"\"\"\n",
    "    Forward pass através de la red de atención usando Numpy\n",
    "\n",
    "    Parametros:\n",
    "    - annotations: Arreglo Numpy con las anotaciones provenientes del Encoder ( shape: batch_size x seq_len x annotation_dim )\n",
    "    - hidden: Arreglo Numpy con los estados ocultos actuales del decoder ( shape: batch_size x hidden_dim )\n",
    "\n",
    "    Retorna:\n",
    "    - attention_weights: Arreglo Numpy con los pesos de atención ( shape: batch_size x seq_len )\n",
    "    \"\"\"\n",
    "\n",
    "    # Expandir los estados ocultos para igualar la dimensión de las anotaciones para la suma de elementos\n",
    "    hidden_expanded = np.expand_dims( hidden, axis = 1 )\n",
    "    hidden_expanded = np.tile( hidden_expanded, ( 1, annotations.shape[1], 1 ) )\n",
    "\n",
    "    # Cálcular los scores de atención\n",
    "    attn_scores = np.dot( np.tanh( np.dot( annotations, self.W1 ) + np.dot( hidden_expanded, self.W2 ) ), self.V )\n",
    "\n",
    "    # Comprimir la ultima dimensión y aplicar softmax para obtener los pesos de atención\n",
    "    attention_weights = softmax( attn_scores.squeeze(-1) )\n",
    "\n",
    "    return attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: (2, 10)\n",
      "[[6.61171973e-01 1.07394587e-03 3.88658447e-30 5.08878893e-42\n",
      "  4.44135299e-08 6.93621233e-11 1.85656613e-30 1.92369569e-01\n",
      "  1.45384468e-01 9.98078514e-17]\n",
      " [2.03947162e-29 6.12211979e-25 1.48010738e-01 1.64206977e-04\n",
      "  8.51801214e-01 1.18565787e-19 3.24040040e-13 4.57690152e-08\n",
      "  4.17307546e-29 2.37950663e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming these dimensions for the sake of example\n",
    "annotation_dim = 256  # Dimension of the encoder's output annotations\n",
    "hidden_dim = 256      # Dimension of the decoder's hidden state\n",
    "attention_dim = 128   # Intermediate attention representation dimension\n",
    "\n",
    "# Initialize the attention network with NumPy\n",
    "attention_network_numpy = AttentionNetworkNumpy(annotation_dim, hidden_dim, attention_dim)\n",
    "\n",
    "# Example annotations and hidden state (randomly generated for demonstration)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "annotations = np.random.randn(batch_size, seq_len, annotation_dim)\n",
    "hidden = np.random.randn(batch_size, hidden_dim)\n",
    "\n",
    "# Forward pass through the attention network\n",
    "attention_weights = attention_network_numpy.forward(annotations, hidden)\n",
    "\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # Expected shape: (batch_size, seq_len)\n",
    "print(attention_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
