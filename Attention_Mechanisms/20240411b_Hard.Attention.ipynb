{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mecanismo de Hard Attention a diferencia de su contraparte \"Soft Attention\" que permite que las gradientes sean propagadas hacia atrás de manera suave al asignar pesos a todas las partes de la entrada. Hard Attention se enfoca selectivamente en partes particulares de los datos de entrada, ignorando el resto. Este enfoque selectivo es parecido a la manera en que los humanos prestán atención a aspectos particulares de una escena visual o pieza de texto mientras ignoran otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características clave  \n",
    "- **Selectividad**: El mecanismo de Hard Attention es selectivo, enfocándose en partes particulares de los datos de entrada. Por ejemplo, en etiquetado de imágenes, el modelo podría enfocarse en objetos en especifico en una imagen en cada paso de la generación de la etiqueta.  \n",
    "  \n",
    "- **No diferenciabilidad**: Hard Attention involucra hacer decisiones discretas, es inherentemente no diferenciable. Esta característica plantea desafios en el entrenamiento usando tecnicas de propagación hacia atrás, que depende de optimización basada en gradientes.  \n",
    "  \n",
    "- **Estocasticidad**: El proceso de selección de Hard Attention es seguido estocastico, significando que involucra cierto nivel de aleatoriedad. Los modelos podrían usar técnicas como muestreo o aprendizaje reforzado para decidir que partes de la entrada enfocarse.  \n",
    "  \n",
    "- **Eficiencia**: Al enfocarse solo en las partes relevantes de la entrada, los mecanismos de Hard Attention pueden ser más eficientes, especialmente con entradas grandes. Ayudan con la carga computacional de procesar y asignar pesos a todas las partes de la entrada en cada paso como lo hace Soft Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retos de entrenamiento\n",
    "\n",
    "Dada la naturaleza no diferenciable de Hard Attention, entrenar modelos que usan este mecanismo puede ser desafiante. Algunas técnicas comunes para abordar este desafío incluyen:\n",
    "\n",
    "- **Muestreo**: En lugar de hacer una selección dura, los modelos pueden muestrear de una distribución de probabilidad para seleccionar partes de la entrada. Esto introduce estocasticidad en el proceso de selección y permite que las gradientes sean propagadas hacia atrás. \n",
    "\n",
    "- **Aprendizaje reforzado**: Los modelos pueden ser entrenados usando técnicas de aprendizaje reforzado, donde se recompensa o penaliza la selección de partes de la entrada basado en el desempeño del modelo. Esto puede ser útil cuando se necesita aprender una política de selección óptima.\n",
    "\n",
    "- **Gumbel-Softmax Trick**: Esta técnica involucra muestrear de una distribución de Gumbel-Softmax, que es una aproximación suave de la distribución de Gumbel. Esto permite que las gradientes sean propagadas hacia atrás y es útil para entrenar modelos con selección dura. \n",
    "\n",
    "- **Monte Carlo Methods**: En lugar de hacer una selección dura, los modelos pueden usar métodos de Monte Carlo para estimar la selección óptima. Esto puede ser útil cuando se necesita una aproximación más suave a la selección dura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas y Algoritmos\n",
    "\n",
    "Hard Attention no sigue una fórmula o algoritmo específico, ya que la selección de partes de la entrada es determinada por el modelo en cada paso. Sin embargo, algunos enfoques comunes para implementar Hard Attention incluyen: \n",
    "\n",
    "- **Selección Dura**: En este enfoque, el modelo selecciona una parte particular de la entrada en cada paso. Esto puede ser implementado usando técnicas como muestreo o aprendizaje reforzado. Como por ejemplo en el caso de etiquetado de imágenes, el modelo podría enfocarse en diferentes regiones de la imagen en cada paso de la generación de la etiqueta. \n",
    "\n",
    "- **Muestreo Estocástico**: En lugar de hacer una selección dura, el modelo puede muestrear de una distribución de probabilidad para seleccionar partes de la entrada. Esto introduce estocasticidad en el proceso de selección y permite que las gradientes sean propagadas hacia atrás.\n",
    "\n",
    "- **Función de decisión**: Los modelos que usan Hard Attention necesitan una función de decisión para determinar que partes de la entrada enfocarse en cada paso. Esta función puede ser aprendida durante el entrenamiento usando técnicas como aprendizaje reforzado o muestreo. \n",
    "\n",
    "- **Salida del modelo**: La salida del modelo en cada paso depende de las partes de la entrada que son seleccionadas por el mecanismo de Hard Attention. Por ejemplo, en el caso de generación de texto, la salida del modelo en cada paso podría depender de las palabras seleccionadas de la entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de aplicación\n",
    "#### Tarea de procesamiento de secuencia con Hard Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HardAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(HardAttention, self).__init__()\n",
    "        # Transforma la dimensión de entrada a la dimensión oculta\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        # Califica cada vector de dimensión oculta (de hidden_dim a 1)\n",
    "        self.scoring_function = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\"\n",
    "        sequence: Un lote de secuencias, forma (tamaño_lote, longitud_secuencia, dim_entrada)\n",
    "        \"\"\"\n",
    "        # Pasa la entrada a través de una capa oculta y una función de activación\n",
    "        hidden_repr = self.activation(self.hidden_layer(sequence))\n",
    "        # Calcula las puntuaciones para cada vector en la secuencia (la forma cambia a (tamaño_lote, longitud_secuencia, 1))\n",
    "        scores = self.scoring_function(hidden_repr).squeeze(-1)\n",
    "        # Convierte las puntuaciones en probabilidades\n",
    "        probabilities = torch.softmax(scores, dim=-1)\n",
    "        # Muestrea un vector por secuencia basado en las puntuaciones\n",
    "        selected_indices = torch.multinomial(probabilities, 1).squeeze(-1)\n",
    "        # Selecciona los vectores basados en los índices muestreados\n",
    "        batch_size, seq_len, _ = sequence.shape\n",
    "        selected_vectors = sequence[torch.arange(batch_size), selected_indices]\n",
    "\n",
    "        return selected_vectors\n",
    "\n",
    "# Ejemplo de uso\n",
    "batch_size, seq_len, input_dim, hidden_dim = 32, 10, 100, 50\n",
    "sequence = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Instancia el modelo\n",
    "hard_attention = HardAttention(input_dim, hidden_dim)\n",
    "selected_vectors = hard_attention(sequence)\n",
    "\n",
    "print(selected_vectors.shape)  # Forma esperada : (batch_size, input_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"El usuario no puede acceder a la red\",\n",
    "    \"Las computadoras no pueden conectarse a la red\",\n",
    "    \"El servidor de correo esta funcionando correctamente\",\n",
    "    \"La laptop no puede cargar la bateria\"\n",
    "]\n",
    "\n",
    "# Funcion ficticia para convertir palabras en embeddings\n",
    "def word_to_embedding(word):\n",
    "    # Para simplificar, esta función genera un vector aleatorio para cada palabra.\n",
    "    # En la práctica, usarías embeddings de palabras reales.\n",
    "    return torch.randn(100)\n",
    "\n",
    "# Convertir oraciones a secuencias de embeddings\n",
    "sequences = [torch.stack([word_to_embedding(word) for word in sentence.split()]) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'El usuario no puede acceder a la red'\n",
      "Summarized by selecting the word: 'la'\n",
      "\n",
      "Sentence: 'Las computadoras no pueden conectarse a la red'\n",
      "Summarized by selecting the word: 'a'\n",
      "\n",
      "Sentence: 'El servidor de correo esta funcionando correctamente'\n",
      "Summarized by selecting the word: 'funcionando'\n",
      "\n",
      "Sentence: 'La laptop no puede cargar la bateria'\n",
      "Summarized by selecting the word: 'cargar'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo de atención dura\n",
    "input_dim = 100  # Dimensión de los embeddings de palabras\n",
    "hidden_dim = 50  # Dimensión oculta para la capa intermedia en el modelo\n",
    "hard_attention = HardAttention(input_dim, hidden_dim)\n",
    "\n",
    "# Procesar cada oración y seleccionar la palabra más importante\n",
    "for i, sequence in enumerate(sequences):\n",
    "    # Agregar una dimensión de lote (tamaño de lote = 1)\n",
    "    sequence = sequence.unsqueeze(0)\n",
    "    # Usar el modelo de atención dura para seleccionar una palabra\n",
    "    selected_vector = hard_attention(sequence)\n",
    "    # Eliminar la dimensión de lote\n",
    "    selected_vector = selected_vector.squeeze(0)\n",
    "    \n",
    "    # Para demostración, solo imprimiremos el índice de la palabra seleccionada\n",
    "    # En una aplicación real, mapearías esto de vuelta a la palabra real\n",
    "    word_index = torch.argmin(torch.sum((sequence.squeeze(0) - selected_vector) ** 2, dim=1))\n",
    "    print(f\"Sentence: '{sentences[i]}'\")\n",
    "    print(f\"Summarized by selecting the word: '{sentences[i].split()[word_index.item()]}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando Hard Attention con Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector seleccionado: [ 3.68628102  6.70469619  9.72311136 12.74152653 15.7599417 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def initialize_parameters( input_dim ):\n",
    "  np.random.seed(42) # Seed para reproducibilidad\n",
    "  weights = np.random.randn( input_dim, 1 ) # Inicialización aleatoria de los pesos\n",
    "  bias = np.random.randn() # Inicialización aleatoria del sesgo\n",
    "  return weights, bias\n",
    "\n",
    "def compute_scores( sequence, weights, bias ):\n",
    "  \"\"\"\n",
    "  Calcular las puntuaciones para cada vector en la secuencia\n",
    "  sequence: Arreglo Numpy de forma ( seq_len, input_dim )\n",
    "  weights: Arreglo Numpy de forma ( input_dim, 1 )\n",
    "  bias: Escalar (flotante)\n",
    "  \"\"\"\n",
    "  scores = np.dot( sequence, weights ) + bias\n",
    "  return scores.squeeze()\n",
    "\n",
    "def select_vector( scores, sequence ):\n",
    "  \"\"\"\n",
    "  Selecciona un vector con la puntuación más alta\n",
    "  scores: Arreglo Numpy de forma ( seq_len, )\n",
    "  sequence: Arreglo Numpy de forma ( seq_len, input_dim )\n",
    "  \"\"\"\n",
    "  selected_index = np.argmax( scores ) # Índice del vector seleccionado\n",
    "  selected_vector = sequence[ selected_index ] # Vector seleccionado\n",
    "  return selected_vector\n",
    "\n",
    "\n",
    "# Ejemplo de secuencia de 5 vectores de 3 dimensiones\n",
    "sequence = np.array(\n",
    "  [\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0]\n",
    "  ]\n",
    ")\n",
    "\n",
    "input_dim = 3 # Dimensión de los vectores de entrada\n",
    "weights, bias = initialize_parameters( input_dim ) # Inicializar los parámetros\n",
    "\n",
    "scores = compute_scores( sequence, weights, bias ) # Calcular las puntuaciones\n",
    "selected_vector = select_vector( scores, sequence ) # Seleccionar el vector con el puntaje más alto\n",
    "\n",
    "print( \"Vector seleccionado:\", scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"Yo amo la programación en Python\",\n",
    "  \"El clima esta demasiado soleado\",\n",
    "  \"El beisbol es un deporte popular en Estados Unidos\",\n",
    "  \"La música es una forma de arte\", \n",
    "  \"El aprendizaje automático es fascinante\"\n",
    "]\n",
    "\n",
    "# Simular la word embedding asignando cada palabra unica un entero ID \n",
    "word_to_id = {\n",
    "  \"yo\": 1,\n",
    "  \"amo\": 2,\n",
    "  \"la\": 3,\n",
    "  \"programación\": 4,\n",
    "  \"en\": 5,\n",
    "  \"python\": 6,\n",
    "  \"el\": 7,\n",
    "  \"clima\": 8,\n",
    "  \"esta\": 9,\n",
    "  \"demasiado\": 10,\n",
    "  \"soleado\": 11,\n",
    "  \"beisbol\": 12,\n",
    "  \"es\": 13,\n",
    "  \"un\": 14,\n",
    "  \"deporte\": 15,\n",
    "  \"popular\": 16,\n",
    "  \"estados\": 17,\n",
    "  \"unidos\": 18,\n",
    "  \"música\": 19,\n",
    "  \"una\": 20,\n",
    "  \"forma\": 21,\n",
    "  \"de\": 22,\n",
    "  \"arte\": 23,\n",
    "  \"aprendizaje\": 24,\n",
    "  \"automático\": 25,\n",
    "  \"fascinante\": 26\n",
    "}\n",
    "\n",
    "# Convertir oraciones a secuencias de embeddings\n",
    "sentence_ids = [[word_to_id[word.lower()] for word in sentence.split()] for sentence in sentences]\n",
    "\n",
    "# Definir manualmente el puntaje para cada ID de palabra ( puntaje más alto indica una mayor importancia )\n",
    "word_scores = {\n",
    "  1: 0.8, 2: 0.7, 3: 0.6, 4: 0.9, 5: 0.5, 6: 0.8,\n",
    "  7: 0.6, 8: 0.4, 9: 0.3, 10: 0.4, 11: 0.2,\n",
    "  12: 0.7, 13: 0.6, 14: 0.5, 15: 0.6, 16: 0.4,\n",
    "  17: 0.5, 18: 0.4, 19: 0.7, 20: 0.6, 21: 0.5,\n",
    "  22: 0.4, 23: 0.8, 24: 0.9, 25: 0.7, 26: 0.8\n",
    "}\n",
    "\n",
    "# Asignar una categoria a cada oración basada en la palabra con el puntaje más alto\n",
    "sentence_categories = {\n",
    "  \"programación\": \"Tecnología\", \n",
    "  \"soleado\": \"Clima\",\n",
    "  \"beisbol\": \"Deportes\",\n",
    "  \"música\": \"Arte\",\n",
    "  \"aprendizaje\": \"Tecnología\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: 'Yo amo la programación en Python'\n",
      "Palabra importante: 'programación', Categoría: 'Tecnología'\n",
      "\n",
      "Oración: 'El clima esta demasiado soleado'\n",
      "Palabra importante: 'el', Categoría: 'Desconocido'\n",
      "\n",
      "Oración: 'El beisbol es un deporte popular en Estados Unidos'\n",
      "Palabra importante: 'beisbol', Categoría: 'Deportes'\n",
      "\n",
      "Oración: 'La música es una forma de arte'\n",
      "Palabra importante: 'arte', Categoría: 'Desconocido'\n",
      "\n",
      "Oración: 'El aprendizaje automático es fascinante'\n",
      "Palabra importante: 'aprendizaje', Categoría: 'Tecnología'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def select_important_word( sentence, word_scores ):\n",
    "  # Convertir la secuencia a puntajes usando el word_scores predefinido\n",
    "  sentence_scores = [word_scores[word_id] for word_id in sentence]\n",
    "\n",
    "  # Usar el mecanismo de selección de Hard Attention\n",
    "  important_word_id = select_vector( np.array( sentence_scores ), np.array( sentence ) )\n",
    "\n",
    "  # Encontrar la palabra correspondiente al id de important_word_id\n",
    "  important_word = [ word for word, id in word_to_id.items() if id == important_word_id ][0]\n",
    "  return important_word\n",
    "\n",
    "# Clasificar cada oración en una categoría basada en la palabra más importante\n",
    "for i, sentence_id in enumerate(sentence_ids):\n",
    "  important_word = select_important_word(sentence_id, word_scores)\n",
    "  category = sentence_categories.get( important_word, \"Desconocido\" )\n",
    "  print( f\"Oración: '{sentences[i]}'\")\n",
    "  print( f\"Palabra importante: '{important_word}', Categoría: '{category}'\\n\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
