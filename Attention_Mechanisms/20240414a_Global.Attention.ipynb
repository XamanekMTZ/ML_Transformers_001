{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Attention\n",
    "\n",
    "Este es un mecanismo de atención que se utiliza para generar una secuencia de salida que corresponde a la secuencia de entrada tal como se hace en traducción automática, resumen de textos y reconocimiento del habla.  \n",
    "  \n",
    "Este mecanismo de atención opera en el principio de permitir que el modelo se enfoque en todas las partes de la secuencia de entrada cuando genera cada elemento de la secuencia de salida. Calcula el vector de contexto por cada salida generada por paso al considerar que toda la secuencia de entrada, al contrario de Local ó Hard Attention que solo se enfocan en una parte de la secuencia de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo funciona?\n",
    "En un modelo tipico de secuencia a secuencia con atención, como en una arquitectura encoder-decoder, el proceso puede ser dividido en varias partes:  \n",
    "  \n",
    "1.- **Encoder**: El encoder es una red neuronal que puede ser RNN, GRU o LSTM que toma la secuencia de entrada y la convierte en una secuencia de vectores de estados ocultos. Cada vector de estado oculto representa una palabra en la secuencia de entrada.  \n",
    "  \n",
    "2.- **Decoder**: Durante la fase de decodificación, por cada paso de tiempo de salida, el modelo usa un decodificador (RNN, GRU o LSTM) para generar el estado oculto y la salida en el paso de tiempo actual. El estado oculto generado en el paso de tiempo actual es usado para calcular el vector de contexto.  \n",
    "  \n",
    "3.- **Pesos de Atención**: Por cada salida en el paso de tiempo actual, el mecanismo de atención global calcula los pesos de atención al comparar el estado oculto actual generador por el decodificador con cada uno de los estados ocultos generados por el encoder. Estos pesos de atención determinan la importancia o relevancia de cada posición de entrada para la salida actual.  \n",
    "  \n",
    "4.- **Vector de Contexto**: Los pesos de atención son usados para crear una suma ponderada de los estados ocultos del encoder, produciendo un vector de contexto. Este vector es una representación dinamica de la secuencia de entrada enfocado en las partes más relevantes del paso de salida actual del proceso de decodificación.  \n",
    "  \n",
    "5.- **Generación de salida**: El vector de contexto es combinado con el estado actual del decoder para generar la salida para el paso de tiempo actual, usualmente a traves de una capa lineal seguida por una función softmax para producir una distribución de probabilidad sobre los posibles tokens de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ventajas\n",
    "\n",
    "- **Flexibilidad**: Permite que el modelo use información desde toda la secuencia de entrada cuando esta generando cada parte de la salida, llevando a una mejor calidad de traducción y resumen de textos.  \n",
    "  \n",
    "- **Interpretabilidad**: Los pesos de atención pueden ser examinados para entender que partes de la secuencia de entrada el modelo se esta enfocando cuando genera partes especificas de la salida, proveyendo una forma de interpretar el proceso de decisión del modelo.  \n",
    "  \n",
    "- **Desempeño mejorado**: Para muchas tareas, particularmente esas que involucran secuencias largas secuencias de entrada o relaciones complejas entre diferentes partes de la entrada y salida, la atención global ha mostrado una mejora significativa en el desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulas Matemáticas\n",
    "\n",
    "1.- **Estados Ocultos Encoder**:  \n",
    "Los estados ocultos producidos por el encoder son representados por la secuencia de entrada $X = (x_1, x_2, ..., x_T)$, donde $T$ es la longitud de la secuencia de entrada. Los estados ocultos son calculados por la red neuronal del encoder y son representados por $H = (h_1, h_2, ..., h_T)$, donde $h_t$ es el estado oculto en el paso de tiempo $t$.  \n",
    "  \n",
    "2.- **Estados Ocultos Decoder en Tiempo $t$**:  \n",
    "Los estados ocultos del decoder en el paso de tiempo $t$ para la secuencia de salida $Y = (y_1, y_2, ..., y_T')$ son representados por $s_t$, donde $T'$ es la longitud de la secuencia de salida.  \n",
    "  \n",
    "3.- **Pesos de Atención en Tiempo $t$**:  \n",
    "El peso de atención $\\alpha_{t, i}$ representa la importancia del estado oculto del encoder $h_i$ para el paso de tiempo $t$ del decoder. Estos pesos son calculados por una función de atención que compara el estado oculto del decoder $s_t$ con cada uno de los estados ocultos del encoder $h_i$.  \n",
    "  \n",
    "$\\alpha_{t, i} = \\frac{exp(f(s_t, h_i))}{\\sum_{j=1}^{T_x} exp(f(s_t, h_j))}$  \n",
    "  \n",
    "La función de puntaje $f$ puede variar, pero una opción común es usar el producto punto o una red neuronal que toma $s_t$ y $h_i$ como entrada.  \n",
    "  \n",
    "4.- **Vector de Contexto en Tiempo $t$**:  \n",
    "El vector de contexto $c_t$ en el tiempo $t$ es la suma ponderada de los estados ocultos del encoder, usando los pesos de atención $\\alpha_{t, i}$ como coeficientes:  \n",
    "  \n",
    "$c_t = \\sum_{i=1}^{T_x} \\alpha_{t, i} h_i$  \n",
    "  \n",
    "5.- **Generación de Salida en Tiempo $t$**:  \n",
    "La salida final $y_t$ en el paso de tiempo $t$ es generada al aplicar la función $g$ (usualmente una capa lineal seguida por una función softmax) a la combinación entre el estado oculto del decoder actual $s_t$ y el vector de contexto $c_t$:  \n",
    "  \n",
    "$y_t = g(s_t, c_t)$  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
