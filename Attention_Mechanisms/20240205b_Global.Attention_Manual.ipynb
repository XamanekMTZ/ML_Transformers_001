{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector Shape: \n",
      "(32, 128)\n",
      "\n",
      "Context Vector:  [[0.46813264 0.78216055 0.46192932 ... 0.39823554 0.78992913 0.76973062]\n",
      " [0.43649398 0.48718817 0.5085277  ... 0.77742532 0.48125993 0.52651599]\n",
      " [0.47534268 0.60960369 0.46723035 ... 0.28381167 0.6513187  0.39213449]\n",
      " ...\n",
      " [0.16946894 0.75410793 0.37903254 ... 0.71693853 0.88752933 0.5679254 ]\n",
      " [0.51978342 0.0029969  0.62563177 ... 0.09780153 0.59340388 0.14625027]\n",
      " [0.47141838 0.22208784 0.58006632 ... 0.54693116 0.42051949 0.54724907]]\n",
      "\n",
      "Attention Weights Shape: \n",
      "(32, 10)\n",
      "\n",
      "Attention Weights:  [[8.18980558e-06 1.47678585e-01 2.90603281e-05 1.21652008e-02\n",
      "  9.63945962e-02 3.76444876e-07 7.19366339e-01 6.71324788e-03\n",
      "  1.76442976e-02 1.07090551e-07]\n",
      " [2.31403755e-02 2.19038604e-02 3.14949327e-04 1.86260763e-01\n",
      "  4.00526551e-01 2.66879992e-01 9.64238359e-02 1.69930892e-03\n",
      "  2.29105545e-03 5.59309100e-04]\n",
      " [3.60357904e-02 2.23946990e-01 5.41057857e-02 1.58290713e-01\n",
      "  3.22000716e-01 9.66439163e-04 1.22939363e-02 3.87683215e-02\n",
      "  7.35417535e-04 1.52855891e-01]\n",
      " [2.23542253e-07 9.90243862e-01 7.84569349e-05 9.29452956e-06\n",
      "  2.71387789e-04 7.05818677e-03 9.84358936e-04 6.57558221e-07\n",
      "  1.17403967e-03 1.79531848e-04]\n",
      " [4.53562696e-06 8.07830073e-03 1.93229308e-03 1.41710305e-02\n",
      "  3.25955313e-04 4.91172968e-05 4.73552715e-04 4.12876265e-01\n",
      "  2.65976286e-02 5.35491322e-01]\n",
      " [8.32492157e-03 4.69556668e-01 2.42325621e-04 5.94881077e-02\n",
      "  4.57297784e-01 4.76160370e-05 1.52863435e-04 2.96035122e-06\n",
      "  4.84842780e-03 3.83259155e-05]\n",
      " [1.50542413e-05 3.69453293e-01 9.28150626e-05 6.06427948e-01\n",
      "  9.23000157e-05 6.02863366e-04 1.87405052e-02 6.61189544e-06\n",
      "  4.56697037e-03 1.63849830e-06]\n",
      " [9.98078118e-01 3.63974748e-05 5.52242381e-04 3.78267953e-06\n",
      "  3.59967332e-04 1.06292447e-07 2.40513403e-06 9.38988474e-04\n",
      "  3.02500747e-06 2.49675798e-05]\n",
      " [1.51319566e-03 1.57915183e-04 1.01339455e-04 1.65042996e-02\n",
      "  6.01644864e-03 6.43090417e-01 2.98722361e-02 2.50602836e-01\n",
      "  3.79167263e-02 1.42245860e-02]\n",
      " [9.75340121e-01 1.22319444e-02 8.06541810e-08 5.96795544e-04\n",
      "  3.47607246e-03 8.82952182e-04 6.82895003e-03 4.39256566e-04\n",
      "  1.45596854e-05 1.89267742e-04]\n",
      " [1.02013840e-05 2.84477811e-03 5.98672094e-01 7.48779390e-04\n",
      "  3.76851331e-01 1.93982713e-05 9.25132988e-04 1.98081202e-02\n",
      "  4.89741260e-05 7.11907830e-05]\n",
      " [1.20121673e-02 3.05334333e-05 2.07962069e-06 1.36214493e-04\n",
      "  9.84634928e-05 7.06946998e-08 7.55002514e-07 1.42181077e-06\n",
      "  9.77584193e-01 1.01341010e-02]\n",
      " [1.84919143e-04 7.89507570e-01 1.14571810e-04 9.13945036e-04\n",
      "  2.51347625e-06 7.65868469e-02 4.57418849e-06 1.29026909e-01\n",
      "  1.52202667e-03 2.13612387e-03]\n",
      " [2.07138522e-04 4.17655135e-02 1.11385384e-03 2.33925237e-05\n",
      "  1.70482904e-04 9.56591188e-01 1.28087269e-06 5.80118936e-06\n",
      "  1.21338230e-04 1.02604204e-08]\n",
      " [5.79803809e-03 8.07907647e-05 5.26494942e-05 2.34207983e-01\n",
      "  7.15708270e-01 2.18007044e-02 1.15365248e-02 4.62521178e-03\n",
      "  6.10895971e-03 8.08679696e-05]\n",
      " [1.74521855e-03 2.04612925e-02 6.84918783e-03 9.95457364e-03\n",
      "  4.24780198e-02 1.22636728e-03 1.26339157e-03 9.05693739e-01\n",
      "  1.03169491e-02 1.12608557e-05]\n",
      " [3.83005325e-04 1.83172004e-04 4.33121918e-03 1.11478882e-06\n",
      "  2.19614948e-05 8.25063431e-01 1.30642044e-01 5.16036166e-07\n",
      "  3.72267489e-05 3.93363092e-02]\n",
      " [4.71382818e-06 6.49412367e-05 1.26222751e-07 1.10817757e-03\n",
      "  2.71929071e-06 1.22100734e-05 3.08425119e-03 1.87366638e-03\n",
      "  9.93778022e-01 7.11725733e-05]\n",
      " [2.23637753e-01 7.86091747e-06 3.10550244e-03 8.42929300e-03\n",
      "  2.64567148e-05 3.33540670e-01 1.21921374e-03 4.95077038e-02\n",
      "  3.89427299e-03 3.76631273e-01]\n",
      " [5.00345284e-02 5.28507175e-01 1.37898792e-03 9.92139467e-03\n",
      "  3.28540949e-01 3.69434044e-03 6.51747598e-02 6.68840170e-03\n",
      "  5.85022562e-03 2.09237468e-04]\n",
      " [1.40112828e-04 2.43232953e-04 1.72048875e-03 1.73876143e-01\n",
      "  7.17500740e-02 6.83134562e-01 3.78217460e-05 1.84741391e-04\n",
      "  2.28554059e-02 4.60574175e-02]\n",
      " [3.79397484e-02 5.51355945e-04 4.32317787e-02 1.02643617e-02\n",
      "  2.00243826e-03 1.12198272e-02 3.96169018e-02 1.33111950e-03\n",
      "  1.62794638e-03 8.52214522e-01]\n",
      " [2.39845363e-01 3.59341225e-05 3.21036516e-03 4.72371130e-04\n",
      "  1.60747829e-07 7.55401215e-01 6.86549678e-04 1.32326038e-06\n",
      "  3.17586854e-04 2.91310314e-05]\n",
      " [2.24249369e-04 5.25753804e-04 4.87000726e-04 2.36944704e-05\n",
      "  9.78628470e-01 2.67893909e-03 1.71167588e-02 3.67997436e-08\n",
      "  8.55928415e-09 3.15088811e-04]\n",
      " [4.69061546e-03 1.72073359e-08 2.03510049e-05 6.77067475e-08\n",
      "  6.78155106e-06 3.26022646e-01 6.54106957e-01 3.74783727e-03\n",
      "  1.10601807e-02 3.44546468e-04]\n",
      " [1.08975081e-03 5.83015268e-05 4.36083786e-04 1.34522581e-01\n",
      "  1.16800303e-02 2.01248063e-04 7.37739745e-01 1.02937172e-01\n",
      "  1.13096280e-02 2.54600342e-05]\n",
      " [2.27053899e-05 2.16621833e-01 6.24156644e-01 9.35537330e-04\n",
      "  9.55524795e-02 1.55771874e-04 2.00383376e-03 1.87483933e-02\n",
      "  1.32290871e-04 4.16705109e-02]\n",
      " [4.75860429e-01 2.00888998e-03 2.28996075e-05 5.00958945e-04\n",
      "  4.90086370e-03 3.24857723e-04 1.08780888e-02 4.43523533e-05\n",
      "  4.82845678e-01 2.26129819e-02]\n",
      " [1.29890967e-02 4.81364673e-03 1.56110752e-01 8.12452712e-01\n",
      "  3.33061091e-05 3.80381288e-04 3.45518585e-03 9.30860594e-03\n",
      "  2.86914079e-04 1.69399393e-04]\n",
      " [9.78105269e-01 6.69700896e-03 8.24428037e-03 2.81663468e-03\n",
      "  3.03784143e-03 3.85553367e-04 5.16863777e-04 1.57097280e-04\n",
      "  2.66294649e-05 1.28221221e-05]\n",
      " [9.04402235e-05 9.94703770e-01 4.19001024e-06 1.84792508e-04\n",
      "  4.39462316e-03 9.17898794e-05 1.88147809e-04 2.15107388e-07\n",
      "  4.97897515e-05 2.92241963e-04]\n",
      " [4.33362157e-01 2.56585229e-05 6.55107676e-03 8.55007329e-05\n",
      "  1.06056174e-05 1.48141711e-03 4.41192121e-01 1.15838316e-05\n",
      "  4.14038605e-04 1.16865841e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Global Attention using only Numpy\n",
    "# reference : https://arxiv.org/pdf/1508.04025.pdf\n",
    "import numpy as np \n",
    "\n",
    "def softmax( x ):\n",
    "  e_x = np.exp( x - np.max( x, axis = -1, keepdims = True ) )\n",
    "  return e_x / np.sum( e_x, axis = -1, keepdims = True )\n",
    "\n",
    "class GlobalAttentionManual:\n",
    "  def __init__( self, input_dim, attention_dim ):\n",
    "    # Initialization of input projection and context vector as Numpy arrays\n",
    "    # as per the paper, the input projection is a matrix of shape ( input_dim, attention_dim )\n",
    "    self.input_projection = np.random.randn( input_dim, attention_dim )\n",
    "    self.context_vector = np.random.randn( attention_dim )\n",
    "\n",
    "  def forward( self, inputs ):\n",
    "    # inputs: ( batch_size, sequence_length, input_dim )\n",
    "\n",
    "    # Projecting the inputs to the attention space\n",
    "    projected_inputs = np.tanh( np.dot( inputs, self.input_projection ) ) # ( batch_size, sequence_length, attention_dim )\n",
    "\n",
    "    # Compute the attention scores using dot product with the context vector\n",
    "    attention_scores = np.dot( projected_inputs, self.context_vector ) # ( batch_size, sequence_length )\n",
    "\n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = softmax( attention_scores ).reshape( attention_scores.shape + ( 1, ) ) # ( batch_size, sequence_length, 1 )\n",
    "\n",
    "    # Compute the context vector as the weighted sum of the inputs\n",
    "    context_vector = np.sum( attention_weights * inputs, axis = 1 ) # ( batch_size, input_dim )\n",
    "\n",
    "    return context_vector, attention_weights.squeeze( -1 )\n",
    "\n",
    "# Example usage\n",
    "input_dim = 128 # input dimension\n",
    "attention_dim = 64 # attention space dimension\n",
    "batch_size = 32 # batch size\n",
    "sequence_length = 10 # sequence length\n",
    "\n",
    "# Example input data \n",
    "inputs = np.random.rand( batch_size, sequence_length, input_dim )\n",
    "\n",
    "# Instantiate the GlobalAttentionManual class and apply the forward pass\n",
    "attention = GlobalAttentionManual( input_dim, attention_dim )\n",
    "context_vector, attention_weights = attention.forward( inputs )\n",
    "\n",
    "print( \"Context Vector Shape: \")\n",
    "print( context_vector.shape ) # ( batch_size, input_dim )\n",
    "print( \"\" )\n",
    "print( \"Context Vector: \", context_vector )\n",
    "print( \"\" )\n",
    "print( \"Attention Weights Shape: \")\n",
    "print( attention_weights.shape ) # ( batch_size, sequence_length )\n",
    "print( \"\" )\n",
    "print( \"Attention Weights: \", attention_weights )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
