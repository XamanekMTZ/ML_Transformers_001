{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Attention Mechanism\n",
    "\n",
    "The next code implements a simple and basic attention mechanism using  \n",
    "PyTorch as the main library. The code is based on the following paper: \n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by Bahdanau et al.  \n",
    "This mechanism can be described as follows:  \n",
    "  \n",
    "1. Initialization (`__init__`): Constructor method that initializes the attention  \n",
    "mechanism using two linear layers.  \n",
    "nn.Linear the first layer is used to transform the input dimension to the  \n",
    "intermediate attention dimension.  \n",
    "The second layer context_vector_layer is used to reduce the intermediate attention  \n",
    "dimension to 1 to obtain a unique attention score per element in the input without  \n",
    "a bias (bias=False).  \n",
    "2. Forward (`forward`): Method that computes the attention scores using the  \n",
    "input tensor received with shape (batch_size, sequence_len, input_dim)  \n",
    "- Step 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:  tensor([[ 0.1510, -0.0671,  0.7450,  ...,  0.2707,  0.2550, -0.1792],\n",
      "        [ 0.5306,  0.6922, -0.4998,  ...,  0.1630,  0.5041, -0.3247],\n",
      "        [ 0.0834,  0.1286,  0.3469,  ..., -0.1087,  0.0986,  0.3567],\n",
      "        ...,\n",
      "        [ 0.0211,  0.4529,  0.1570,  ..., -0.1851, -0.0764,  0.0087],\n",
      "        [-0.0842, -0.0134,  0.0250,  ..., -0.0995,  0.0669, -0.3522],\n",
      "        [ 0.0158, -0.0941,  0.3257,  ...,  0.0744,  0.0198,  0.3272]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "\n",
      "Attention Weights:  tensor([[[0.0925],\n",
      "         [0.1648],\n",
      "         [0.0810],\n",
      "         [0.1057],\n",
      "         [0.1073],\n",
      "         [0.0779],\n",
      "         [0.0686],\n",
      "         [0.0884],\n",
      "         [0.1309],\n",
      "         [0.0830]],\n",
      "\n",
      "        [[0.1150],\n",
      "         [0.0758],\n",
      "         [0.1068],\n",
      "         [0.0942],\n",
      "         [0.1265],\n",
      "         [0.0844],\n",
      "         [0.0900],\n",
      "         [0.1485],\n",
      "         [0.0642],\n",
      "         [0.0945]],\n",
      "\n",
      "        [[0.0995],\n",
      "         [0.0634],\n",
      "         [0.0989],\n",
      "         [0.1063],\n",
      "         [0.0832],\n",
      "         [0.1068],\n",
      "         [0.0926],\n",
      "         [0.1001],\n",
      "         [0.1586],\n",
      "         [0.0906]],\n",
      "\n",
      "        [[0.0959],\n",
      "         [0.0722],\n",
      "         [0.1301],\n",
      "         [0.0969],\n",
      "         [0.1241],\n",
      "         [0.0521],\n",
      "         [0.1244],\n",
      "         [0.0926],\n",
      "         [0.1307],\n",
      "         [0.0812]],\n",
      "\n",
      "        [[0.1008],\n",
      "         [0.1298],\n",
      "         [0.0979],\n",
      "         [0.0704],\n",
      "         [0.0652],\n",
      "         [0.1568],\n",
      "         [0.0843],\n",
      "         [0.1180],\n",
      "         [0.0982],\n",
      "         [0.0785]],\n",
      "\n",
      "        [[0.0788],\n",
      "         [0.0954],\n",
      "         [0.1247],\n",
      "         [0.1053],\n",
      "         [0.0843],\n",
      "         [0.0735],\n",
      "         [0.0824],\n",
      "         [0.0952],\n",
      "         [0.1025],\n",
      "         [0.1577]],\n",
      "\n",
      "        [[0.0713],\n",
      "         [0.1079],\n",
      "         [0.1154],\n",
      "         [0.1334],\n",
      "         [0.0563],\n",
      "         [0.1288],\n",
      "         [0.1023],\n",
      "         [0.0970],\n",
      "         [0.1086],\n",
      "         [0.0791]],\n",
      "\n",
      "        [[0.0937],\n",
      "         [0.1340],\n",
      "         [0.1137],\n",
      "         [0.0851],\n",
      "         [0.0805],\n",
      "         [0.0884],\n",
      "         [0.1120],\n",
      "         [0.0901],\n",
      "         [0.1039],\n",
      "         [0.0986]],\n",
      "\n",
      "        [[0.1151],\n",
      "         [0.0814],\n",
      "         [0.1040],\n",
      "         [0.1158],\n",
      "         [0.1333],\n",
      "         [0.1022],\n",
      "         [0.0615],\n",
      "         [0.0838],\n",
      "         [0.0657],\n",
      "         [0.1373]],\n",
      "\n",
      "        [[0.1369],\n",
      "         [0.0779],\n",
      "         [0.0749],\n",
      "         [0.1005],\n",
      "         [0.0949],\n",
      "         [0.0843],\n",
      "         [0.0945],\n",
      "         [0.1072],\n",
      "         [0.0978],\n",
      "         [0.1310]],\n",
      "\n",
      "        [[0.1836],\n",
      "         [0.0621],\n",
      "         [0.0542],\n",
      "         [0.0904],\n",
      "         [0.0988],\n",
      "         [0.1177],\n",
      "         [0.0968],\n",
      "         [0.0941],\n",
      "         [0.0951],\n",
      "         [0.1072]],\n",
      "\n",
      "        [[0.0782],\n",
      "         [0.0746],\n",
      "         [0.1052],\n",
      "         [0.1017],\n",
      "         [0.1191],\n",
      "         [0.0944],\n",
      "         [0.0700],\n",
      "         [0.0948],\n",
      "         [0.1428],\n",
      "         [0.1193]],\n",
      "\n",
      "        [[0.0974],\n",
      "         [0.1025],\n",
      "         [0.0664],\n",
      "         [0.0998],\n",
      "         [0.1079],\n",
      "         [0.1389],\n",
      "         [0.1112],\n",
      "         [0.0743],\n",
      "         [0.0916],\n",
      "         [0.1099]],\n",
      "\n",
      "        [[0.0888],\n",
      "         [0.1171],\n",
      "         [0.0718],\n",
      "         [0.0889],\n",
      "         [0.1132],\n",
      "         [0.1060],\n",
      "         [0.0969],\n",
      "         [0.1061],\n",
      "         [0.0953],\n",
      "         [0.1159]],\n",
      "\n",
      "        [[0.1197],\n",
      "         [0.1105],\n",
      "         [0.0868],\n",
      "         [0.0775],\n",
      "         [0.0811],\n",
      "         [0.0987],\n",
      "         [0.0899],\n",
      "         [0.1292],\n",
      "         [0.1296],\n",
      "         [0.0773]],\n",
      "\n",
      "        [[0.0848],\n",
      "         [0.1186],\n",
      "         [0.0719],\n",
      "         [0.1179],\n",
      "         [0.1212],\n",
      "         [0.0834],\n",
      "         [0.1109],\n",
      "         [0.1356],\n",
      "         [0.0856],\n",
      "         [0.0701]],\n",
      "\n",
      "        [[0.0635],\n",
      "         [0.0971],\n",
      "         [0.1199],\n",
      "         [0.1253],\n",
      "         [0.0658],\n",
      "         [0.0828],\n",
      "         [0.1133],\n",
      "         [0.1380],\n",
      "         [0.0848],\n",
      "         [0.1096]],\n",
      "\n",
      "        [[0.1437],\n",
      "         [0.1330],\n",
      "         [0.0885],\n",
      "         [0.0823],\n",
      "         [0.0973],\n",
      "         [0.0982],\n",
      "         [0.0704],\n",
      "         [0.0858],\n",
      "         [0.0774],\n",
      "         [0.1235]],\n",
      "\n",
      "        [[0.0817],\n",
      "         [0.1025],\n",
      "         [0.0940],\n",
      "         [0.0710],\n",
      "         [0.1149],\n",
      "         [0.1231],\n",
      "         [0.1444],\n",
      "         [0.1137],\n",
      "         [0.0769],\n",
      "         [0.0777]],\n",
      "\n",
      "        [[0.1324],\n",
      "         [0.1075],\n",
      "         [0.0885],\n",
      "         [0.0804],\n",
      "         [0.0921],\n",
      "         [0.0902],\n",
      "         [0.1436],\n",
      "         [0.0795],\n",
      "         [0.1225],\n",
      "         [0.0633]],\n",
      "\n",
      "        [[0.0880],\n",
      "         [0.1100],\n",
      "         [0.1092],\n",
      "         [0.0818],\n",
      "         [0.0666],\n",
      "         [0.0946],\n",
      "         [0.1146],\n",
      "         [0.1110],\n",
      "         [0.0759],\n",
      "         [0.1484]],\n",
      "\n",
      "        [[0.0637],\n",
      "         [0.0723],\n",
      "         [0.1474],\n",
      "         [0.0925],\n",
      "         [0.1337],\n",
      "         [0.0988],\n",
      "         [0.0740],\n",
      "         [0.1064],\n",
      "         [0.1198],\n",
      "         [0.0914]],\n",
      "\n",
      "        [[0.1426],\n",
      "         [0.1081],\n",
      "         [0.1040],\n",
      "         [0.0550],\n",
      "         [0.0680],\n",
      "         [0.0912],\n",
      "         [0.1341],\n",
      "         [0.1120],\n",
      "         [0.1070],\n",
      "         [0.0783]],\n",
      "\n",
      "        [[0.1126],\n",
      "         [0.1075],\n",
      "         [0.1151],\n",
      "         [0.0858],\n",
      "         [0.0893],\n",
      "         [0.0899],\n",
      "         [0.0832],\n",
      "         [0.1097],\n",
      "         [0.0840],\n",
      "         [0.1230]],\n",
      "\n",
      "        [[0.0691],\n",
      "         [0.0711],\n",
      "         [0.1013],\n",
      "         [0.1203],\n",
      "         [0.1391],\n",
      "         [0.0940],\n",
      "         [0.0606],\n",
      "         [0.0871],\n",
      "         [0.1329],\n",
      "         [0.1245]],\n",
      "\n",
      "        [[0.1268],\n",
      "         [0.0953],\n",
      "         [0.1182],\n",
      "         [0.0811],\n",
      "         [0.0996],\n",
      "         [0.0829],\n",
      "         [0.0746],\n",
      "         [0.1375],\n",
      "         [0.0975],\n",
      "         [0.0865]],\n",
      "\n",
      "        [[0.1381],\n",
      "         [0.0917],\n",
      "         [0.1136],\n",
      "         [0.0916],\n",
      "         [0.0876],\n",
      "         [0.1200],\n",
      "         [0.0839],\n",
      "         [0.1000],\n",
      "         [0.0760],\n",
      "         [0.0973]],\n",
      "\n",
      "        [[0.0810],\n",
      "         [0.0640],\n",
      "         [0.1257],\n",
      "         [0.1323],\n",
      "         [0.0902],\n",
      "         [0.0846],\n",
      "         [0.0845],\n",
      "         [0.1290],\n",
      "         [0.1344],\n",
      "         [0.0743]],\n",
      "\n",
      "        [[0.1245],\n",
      "         [0.1047],\n",
      "         [0.0726],\n",
      "         [0.0720],\n",
      "         [0.0962],\n",
      "         [0.0693],\n",
      "         [0.1324],\n",
      "         [0.1026],\n",
      "         [0.0934],\n",
      "         [0.1322]],\n",
      "\n",
      "        [[0.0995],\n",
      "         [0.1361],\n",
      "         [0.1142],\n",
      "         [0.0855],\n",
      "         [0.0670],\n",
      "         [0.1139],\n",
      "         [0.0937],\n",
      "         [0.0714],\n",
      "         [0.1257],\n",
      "         [0.0930]],\n",
      "\n",
      "        [[0.0964],\n",
      "         [0.0722],\n",
      "         [0.0957],\n",
      "         [0.1558],\n",
      "         [0.0941],\n",
      "         [0.1178],\n",
      "         [0.1124],\n",
      "         [0.0675],\n",
      "         [0.0894],\n",
      "         [0.0987]],\n",
      "\n",
      "        [[0.0623],\n",
      "         [0.1236],\n",
      "         [0.1159],\n",
      "         [0.1198],\n",
      "         [0.0811],\n",
      "         [0.1027],\n",
      "         [0.0838],\n",
      "         [0.1063],\n",
      "         [0.1435],\n",
      "         [0.0610]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicAttention( nn.Module ):\n",
    "  def __init__( self, input_dim, attention_dim ):\n",
    "    super( BasicAttention, self ).__init__()\n",
    "    self.attention_weights_layer = nn.Linear( input_dim, attention_dim )\n",
    "    self.context_vector_layer = nn.Linear( attention_dim, 1, bias = False )\n",
    "\n",
    "  def forward( self, inputs ):\n",
    "    # inputs: ( batch_size, sequence_length, input_dim )\n",
    "\n",
    "    # Step 1: Calculate attention weights \n",
    "    attention_scores = self.attention_weights_layer( inputs ) # Transform inputs to attention space ( batch_size, sequence_length, attention_dim )\n",
    "    attention_scores = torch.tanh( attention_scores ) # Apply non-linearity function \n",
    "    attention_weights = self.context_vector_layer( attention_scores ) # Calculates attention score for each input \n",
    "    attention_weights = torch.softmax( attention_weights.squeeze( -1 ), dim = -1 ).unsqueeze( -1 ) # Apply softmax to get normalized attention weights and prepares for multiplication\n",
    "\n",
    "    # Step 2: Apply attention weights to inputs to get context vector\n",
    "    context_vector = torch.sum( attention_weights * inputs, dim = 1 ) # Multiply inputs by attention weights and sum over sequence length to get context vector\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "  \n",
    "# Example\n",
    "batch_size = 1 \n",
    "sequence_length = 5\n",
    "input_dim = 10\n",
    "attention_dim = 20 \n",
    "  \n",
    "# input_dim = 128 # Dimension of the input data\n",
    "# attention_dim = 64 # Dimension of the attention space\n",
    "# batch_size = 32 # Batch size\n",
    "# sequence_length = 10 # Sequence length\n",
    "\n",
    "inputs = torch.randn( batch_size, sequence_length, input_dim )\n",
    "attention_layer = BasicAttention( input_dim, attention_dim )\n",
    "context_vector, attention_weights = attention_layer( inputs )\n",
    "\n",
    "print( \"Context Vector: \", context_vector )\n",
    "print( \"\" )\n",
    "print( \"Attention Weights: \", attention_weights )"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Updated Code for Soft Attention (Basic attention)\n",
    "# The following code implements a simple and basic attention mechanism using PyTorch as the main library.\n",
    "# The code is based on the following paper: Neural Machine Translation by Jointly Learning to Align and Translate by Bahdanau et al.\n",
    "# 10 - 04 - 2024\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionNetwork(nn.Module):\n",
    "    def __init__(self, annotation_dim, hidden_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        Initialize the attention network.\n",
    "        \n",
    "        Parameters:\n",
    "        - annotation_dim: Dimension of the input annotations (a)\n",
    "        - hidden_dim: Dimension of the decoder's hidden state (h_0)\n",
    "        - attention_dim: Dimension for the intermediate representation in the attention mechanism\n",
    "        \"\"\"\n",
    "        super(AttentionNetwork, self).__init__()\n",
    "        \n",
    "        # Define the weight matrices\n",
    "        self.W1 = nn.Linear(annotation_dim, attention_dim, bias=False)  # Applies W1 to annotation a\n",
    "        self.W2 = nn.Linear(hidden_dim, attention_dim, bias=False)  # Applies W2 to hidden state h0\n",
    "        self.V = nn.Linear(attention_dim, 1, bias=False)  # Applies V to the tanh output\n",
    "\n",
    "    def forward(self, annotations, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through the attention network.\n",
    "        \n",
    "        Parameters:\n",
    "        - annotations: Tensor containing annotations from the encoder (shape: batch_size x seq_len x annotation_dim)\n",
    "        - hidden: Tensor containing the current hidden state of the decoder (shape: batch_size x hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - attention_weights: Tensor containing the attention weights (shape: batch_size x seq_len)\n",
    "        \"\"\"\n",
    "        # Expand hidden state to match the dimensions of annotations for element-wise addition\n",
    "        hidden = hidden.unsqueeze(1).expand_as(annotations)\n",
    "        \n",
    "        # Compute the attention scores\n",
    "        attn_scores = self.V(torch.tanh(self.W1(annotations) + self.W2(hidden)))\n",
    "        \n",
    "        # Squeeze the last dimension and apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attn_scores.squeeze(-1), dim=-1)\n",
    "        \n",
    "        return attention_weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T17:06:39.551998Z",
     "start_time": "2024-04-10T17:06:35.880459Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming these dimensions for the sake of example\n",
    "annotation_dim = 256  # Dimension of the encoder's output annotations\n",
    "hidden_dim = 256      # Dimension of the decoder's hidden state\n",
    "attention_dim = 128   # Intermediate attention representation dimension\n",
    "\n",
    "# Initialize the attention network\n",
    "attention_network = AttentionNetwork(annotation_dim, hidden_dim, attention_dim)\n",
    "\n",
    "# Example annotations and hidden state (randomly generated for demonstration)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "annotations = torch.randn(batch_size, seq_len, annotation_dim)\n",
    "hidden = torch.randn(batch_size, hidden_dim)\n",
    "\n",
    "# Forward pass through the attention network\n",
    "attention_weights = attention_network(annotations, hidden)\n",
    "\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # Expected shape: (batch_size, seq_len)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T17:06:42.265394Z",
     "start_time": "2024-04-10T17:06:42.148871Z"
    }
   },
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
