{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:  [[0.36984988 0.61671825 0.31822509 0.60165173 0.5588019  0.35451691\n",
      "  0.63199326 0.21645852 0.80540425 0.67249008]]\n",
      "\n",
      "Attention Weights:  [[0.19937268 0.19023509 0.19817685 0.20136077 0.21085462]]\n"
     ]
    }
   ],
   "source": [
    "# Implements a basic attention mechanism without using PyTorch\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def linear_transform( X, W, b = None ):\n",
    "  \"\"\"\n",
    "  Performs a linear transformation of the data: Y = XW + b\n",
    "  :param X: Input data, NumPy array of shape (batch_size, ..., input_dim)\n",
    "  :param W: Weight matrix, NumPy array of shape (input_dim, output_dim)\n",
    "  :param b: Bias vector, NumPy array of shape (output_dim,) or None\n",
    "  :return: NumPy array with the transformed data\n",
    "  \"\"\"\n",
    "  Y = np.dot( X, W )\n",
    "  if b is not None:\n",
    "    Y += b\n",
    "  return Y\n",
    "\n",
    "def basic_attention( inputs, W1, b1, W2 ):\n",
    "  \"\"\"\n",
    "  Implements a basic attention mechanism without using PyTorch.\n",
    "  :param inputs: Input data, NumPy array of shape (batch_size, sequence_length, input_dim)\n",
    "  :param W1: Weight matrix for the attention_weights_layer layer, of shape (input_dim, attention_dim)\n",
    "  :param b1: Bias vector for the attention_weights_layer layer, of shape (attention_dim,)\n",
    "  :param W2: Weight matrix for the context_vector_layer, of shape (attention_dim, 1)\n",
    "  :return: Tuple of context_vector and attention_weights\n",
    "  \"\"\"\n",
    "  # Step 1: Compute attention weights\n",
    "  # Transform inputs to attention space \n",
    "  attention_scores = linear_transform( inputs, W1, b1 )\n",
    "  # Apply non-linearity to attention scores ( tanh )\n",
    "  attention_scores = np.tanh( attention_scores )\n",
    "  # Compute attention scores for each input \n",
    "  attention_weights = linear_transform( attention_scores, W2 )\n",
    "  # Apply softmax to obtain normalized attention weights\n",
    "  attention_weights = np.exp( attention_weights - np.max( attention_weights, axis = 1, keepdims = True ) )\n",
    "  attention_weights /= np.sum( attention_weights, axis = 1, keepdims = True )\n",
    "\n",
    "  # Step 2: Apply attention weights to inputs to obtain context vector\n",
    "  context_vector = np.sum( attention_weights * inputs, axis = 1 )\n",
    "\n",
    "  return context_vector, attention_weights.squeeze( -1 )\n",
    "\n",
    "# Dimension and parameters ( for testing )\n",
    "batch_size = 1\n",
    "sequence_length = 5\n",
    "input_dim = 10\n",
    "attention_dim = 20\n",
    "\n",
    "# Initialization of parameters\n",
    "W1 = np.random.rand( input_dim, attention_dim )\n",
    "b1 = np.random.rand( attention_dim )\n",
    "W2 = np.random.rand( attention_dim, 1 )\n",
    "\n",
    "# Example input data ( random )\n",
    "inputs = np.random.rand( batch_size, sequence_length, input_dim )\n",
    "\n",
    "context_vector, attention_weights = basic_attention( inputs, W1, b1, W2 )\n",
    "\n",
    "print( \"Context Vector: \", context_vector ) \n",
    "print(\"\")\n",
    "print( \"Attention Weights: \", attention_weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:  [[0.47855559 0.50177232 0.28462834 ... 0.61515627 0.63151555 0.52124213]\n",
      " [0.54984241 0.50824661 0.452511   ... 0.44746394 0.39663317 0.48986191]\n",
      " [0.47734344 0.46148235 0.58379601 ... 0.50083603 0.43596871 0.49653411]\n",
      " ...\n",
      " [0.37177787 0.37541966 0.42256784 ... 0.7021436  0.62209681 0.38691504]\n",
      " [0.5438249  0.43593613 0.55316309 ... 0.63541769 0.4919232  0.62900256]\n",
      " [0.51594188 0.44686249 0.46423829 ... 0.48957873 0.4263816  0.54310004]]\n",
      "\n",
      "Attention Weights:  [[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of the basic attention mechanism without using PyTorch\n",
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax( x ):\n",
    "  # Subtract the max for numerical stability\n",
    "  e_x = np.exp( x - np.max( x, axis = -1, keepdims = True ) )\n",
    "  # Normalize\n",
    "  return e_x / e_x.sum( axis = -1, keepdims = True )\n",
    "\n",
    "class BasicAttentionManual:\n",
    "  def __init__( self, input_dim, attention_dim ):\n",
    "    # Random initialization of the weight matrices\n",
    "    self.W_att = np.random.rand( input_dim, attention_dim )\n",
    "    self.W_ctx = np.random.rand( attention_dim, 1 )\n",
    "\n",
    "  def forward( self, inputs ):\n",
    "    # Step 1: Compute attention weights\n",
    "    # Transform inputs to attention space\n",
    "    attention_scores = np.tanh( np.dot( inputs, self.W_att ) )\n",
    "    # Compute attention scores for each input \n",
    "    attention_scores = np.dot( attention_scores, self.W_ctx )\n",
    "    # Apply softmax to obtain normalized attention weights\n",
    "    attention_weights = softmax( attention_scores.squeeze( -1 ) ).reshape( attention_scores.shape )\n",
    "\n",
    "    # Step 2: Apply attention weights to inputs to obtain context vector\n",
    "    context_vector = np.sum( attention_weights * inputs, axis = 1 )\n",
    "\n",
    "    return context_vector, attention_weights.squeeze( -1 )\n",
    "  \n",
    "input_dim = 128 # Dimension of the input data\n",
    "attention_dim = 64 # Dimension of the attention space\n",
    "batch_size = 32 # Batch size\n",
    "sequence_length = 10 # Sequence length\n",
    "\n",
    "# example input data ( random ) \n",
    "inputs = np.random.rand( batch_size, sequence_length, input_dim )\n",
    "\n",
    "attention = BasicAttentionManual( input_dim, attention_dim )\n",
    "context_vector, attention_weights = attention.forward( inputs )\n",
    "\n",
    "print( \"Context Vector: \", context_vector )\n",
    "print(\"\")\n",
    "print( \"Attention Weights: \", attention_weights )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
