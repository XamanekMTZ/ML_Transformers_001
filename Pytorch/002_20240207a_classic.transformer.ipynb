{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a Transformer model, we will follow the next steps:  \n",
    "  \n",
    "1. Import necessary libraries and modules  \n",
    "2. Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding  \n",
    "3. Build the Encoder and Decoder Layers\n",
    "4. Combine Encoder and Decoder layers to create the complete Transformer Model  \n",
    "5. Prepare sample data  \n",
    "6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the necessary libraries and modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to define the building blocks of the Transformer model.\n",
    "### Multi-Head Attention mechanism\n",
    "The Multi-Head Attention mechanism computes the attention between each pair of  \n",
    "positions in a sequence. It consists of multiple \"attention heads\" that capture  \n",
    "different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention( nn.Module ):\n",
    "  def __init__( self, d_model, num_heads ):\n",
    "    super( MultiHeadAttention, self ).__init__()\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.W_q = nn.Linear( d_model, d_model )\n",
    "    self.W_k = nn.Linear( d_model, d_model )\n",
    "    self.W_v = nn.Linear( d_model, d_model )\n",
    "    self.W_o = nn.Linear( d_model, d_model )\n",
    "\n",
    "  def scaled_dot_product_attention( self, Q, K, V, mask = None ):\n",
    "    attn_scores = torch.matmul( Q, K.transpose( -2, -1 ) ) / math.sqrt( self.d_k )\n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill( mask == 0, -1e9 )\n",
    "    attn_probs = torch.softmax( attn_scores, dim = -1 )\n",
    "    output = torch.matmul( attn_probs, V )\n",
    "    return output\n",
    "\n",
    "  def split_heads( self, x ):\n",
    "    batch_size, seq_length, d_model = x.size()\n",
    "    return x.view( batch_size, seq_length, self.num_heads, self.d_k ).transpose( 1, 2 )\n",
    "\n",
    "  def combine_heads( self, x ):\n",
    "    batch_size, _, seq_length, d_k = x.size()\n",
    "    return x.transpose( 1, 2 ).contiguous().view( batch_size, seq_length, self.d_model )\n",
    "\n",
    "  def forward( self, Q, K, V, mask = None ):\n",
    "    Q = self.split_heads( self.W_q( Q ) )\n",
    "    K = self.split_heads( self.W_k( K ) )\n",
    "    V = self.split_heads( self.W_v( V ) )\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention( Q, K, V, mask )\n",
    "    output = self.W_o( self.combine_heads( attn_output ) )\n",
    "    return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiHeadAttention code initializes the module with input parameters and  \n",
    "linear transformation layers. It calculates attention scores, reshapes the  \n",
    "input tensor into multiple heads, and combines the attention outputs from  \n",
    "all heads. The forward method computes the multi-head self-attention,  \n",
    "allowing the model to focus on some different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "The PositionWiseFeedForward class extends PyTorch's nn.Module and implements a  \n",
    "position-wise feed-forward network. The class initializes with two linear  \n",
    "transformation layers and a ReLU activation function. The forward method applies  \n",
    "these transformations and activation function sequentially to compute the output.  \n",
    "This process enables the model to consider the position of input elements  \n",
    "while making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward( nn.Module ):\n",
    "  def __init__( self, d_model, d_ff ):\n",
    "    super( PositionWiseFeedForward, self ).__init__()\n",
    "    self.fc1 = nn.Linear( d_model, d_ff )\n",
    "    self.fc2 = nn.Linear( d_ff, d_model )\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward( self, x ):\n",
    "    return self.fc2( self.relu( self.fc1( x ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Positional Encoding is used to inject the position information of each token  \n",
    "in the input sequence. It uses sine and cosine functions of different  \n",
    "frequencies to generate the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding( nn.Module ):\n",
    "  def __init__( self, d_model, max_seq_length ):\n",
    "    super( PositionalEncoding, self ).__init__()\n",
    "\n",
    "    pe = torch.zeros( max_seq_length, d_model )\n",
    "    position = torch.arange( 0, max_seq_length, dtype = torch.float ).unsqueeze( 1 )\n",
    "    div_term = torch.exp( torch.arange( 0, d_model, 2 ).float() * -( math.log( 10000.0 ) / d_model ) )\n",
    "\n",
    "    pe[ :, 0::2 ] = torch.sin( position * div_term )\n",
    "    pe[ :, 1::2 ] = torch.cos( position * div_term )\n",
    "\n",
    "    self.register_buffer( 'pe', pe.unsqueeze( 0 ) )\n",
    "\n",
    "  def forward( self, x ):\n",
    "    return x + self.pe[ :, :x.size( 1 ) ]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PositionalEncoding class initializes with input parameters d_model and  \n",
    "max_seq_length, creating a tensor to store positional encoding values. The class  \n",
    "calculates sine and cosine values for even and odd indices, respectively, based on the  \n",
    "scaling factor div_term. The forward method computes the positional encoding by adding  \n",
    "the stored positional encoding values to the input tensor, allowing the model to capture  \n",
    "the position information of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward  \n",
    "layer, and two Layer Normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "  def __init__ (self, d_model, num_heads, d_ff, dropout ):\n",
    "    super( EncoderLayer, self ).__init__()\n",
    "    self.self_attn = MultiHeadAttention( d_model, num_heads )\n",
    "    self.feed_forward = PositionWiseFeedForward( d_model, d_ff )\n",
    "    self.norm1 = nn.LayerNorm( d_model )\n",
    "    self.norm2 = nn.LayerNorm( d_model )\n",
    "    self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "  def forward( self, x, mask ):\n",
    "    attn_output = self.self_attn( x, x, x, mask )\n",
    "    x = self.norm1( x + self.dropout( attn_output ) )\n",
    "    ff_output = self.feed_forward( x )\n",
    "    x = self.norm2( x + self.dropout( ff_output ) )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class initializes with input parameters and components, including  \n",
    "a MultiHeadAttention module, a PositionWiseFeedForward module, two layer  \n",
    "normalization modules, and a dropout layer. The forward methods computes the encoder  \n",
    "layer output by applying self-attention, adding the attention output to the input  \n",
    "tensor, and normalizing the result. Then, it computes the position-wise feed-forward  \n",
    "output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "This layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward  \n",
    "layer, and three Layer Normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer( nn.Module ):\n",
    "  def __init__( self, d_model, num_heads, d_ff, dropout ):\n",
    "    super( DecoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention( d_model, num_heads )\n",
    "    self.cross_attn = MultiHeadAttention( d_model, num_heads )\n",
    "    self.feed_forward = PositionWiseFeedForward( d_model, d_ff )\n",
    "    self.norm1 = nn.LayerNorm( d_model )\n",
    "    self.norm2 = nn.LayerNorm( d_model )\n",
    "    self.norm3 = nn.LayerNorm( d_model )\n",
    "    self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "  def forward( self, x, enc_output, src_mask, tgt_mask ):\n",
    "    attn_output = self.self_attn( x, x, x, tgt_mask )\n",
    "    x = self.norm1( x + self.dropout( attn_output ) )\n",
    "    attn_output = self.cross_attn( x, enc_output, enc_output, src_mask )\n",
    "    x = self.norm2( x + self.dropout( attn_output ) )\n",
    "    ff_output = self.feed_forward( x )\n",
    "    x = self.norm3( x + self.dropout( ff_output ) )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecoderLayer initializes with input parameters and components such as   \n",
    "MultiHeadAttentio modules for masked self-attention and corss-attention, a   \n",
    "PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.  \n",
    "  \n",
    "The forward method computes the decoder layer output by performing the following steps:  \n",
    "  \n",
    "1. Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.  \n",
    "  \n",
    "2. Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized mask self-attention output, followed by dropout and layer normalization.  \n",
    "  \n",
    "3. Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.\n",
    "  \n",
    "4. Return the processed tensor.  \n",
    "  \n",
    "These operations enable the decoder to generate target sequences based on the input and the encoder output.  \n",
    "  \n",
    "Now we can combine the Encoder and Decoder layers to create the complete Transformer model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer( nn.Module ):\n",
    "  def __init__( self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout ):\n",
    "    super( Transformer, self ).__init__()\n",
    "    self.encoder_embedding = nn.Embedding( src_vocab_size, d_model )\n",
    "    self.decoder_embedding = nn.Embedding( tgt_vocab_size, d_model )\n",
    "    self.positional_encoding = PositionalEncoding( d_model, max_seq_length )\n",
    "\n",
    "    self.encoder_layers = nn.ModuleList( \n",
    "      [ EncoderLayer( \n",
    "        d_model, \n",
    "        num_heads, \n",
    "        d_ff, \n",
    "        dropout ) for _ in range( num_layers) \n",
    "      ] \n",
    "    )\n",
    "\n",
    "    self.decoder_layers = nn.ModuleList(\n",
    "      [ DecoderLayer(\n",
    "        d_model, \n",
    "        num_heads, \n",
    "        d_ff, \n",
    "        dropout ) for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear( d_model, tgt_vocab_size )\n",
    "    self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "  def generate_mask( self, src, tgt ):\n",
    "    src_mask = ( src != 0 ).unsqueeze( 1 ).unsqueeze( 2 )\n",
    "    tgt_mask = ( tgt != 0 ).unsqueeze( 1 ).unsqueeze( 3 )\n",
    "    seq_length = tgt.size( 1 )\n",
    "    nopeak_mask = ( \n",
    "      1 - torch.triu( \n",
    "        torch.ones( 1, seq_length, seq_length ), \n",
    "        diagonal = 1 \n",
    "      ) \n",
    "    ).bool()\n",
    "\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, tgt_mask\n",
    "  \n",
    "  def forward( self, src, tgt ):\n",
    "    src_mask, tgt_mask = self.generate_mask( src, tgt )\n",
    "    src_embedded = self.dropout( \n",
    "      self.positional_encoding( self.encoder_embedding( src ) ) \n",
    "    )\n",
    "    tgt_embedded = self.dropout(\n",
    "      self.positional_encoding( self.decoder_embedding( tgt ) )\n",
    "    )\n",
    "\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "      enc_output = enc_layer( enc_output, src_mask )\n",
    "\n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      dec_output = dec_layer( dec_output, enc_output, src_mask, tgt_mask )\n",
    "\n",
    "    output = self.fc( dec_output )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines the previously defined modules to create a complete model.  \n",
    "During initializacion, the Transformer module sets up input parameters and  \n",
    "initializes various components, including embedding layers for source and target  \n",
    "sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules  \n",
    "to create stacked layers, a linear layer for projecting decoder output, and a  \n",
    "dropout layer.\n",
    "\n",
    "The 'generate_mask' method creates binary masks for source and target sequences  \n",
    "to ignore padding tokens and prevent the decoder from attenting to future tokens.  \n",
    "The forward method computes the Transformer model's output through the following steps:  \n",
    "  \n",
    "1. Generate source and target masks using the 'generate_mask' method.  \n",
    "  \n",
    "2. Compute source and target embeddings, and apply positional encoding and dropout.  \n",
    "  \n",
    "3. Process the source sequence through encoder layers, updating the enc_output tensor.  \n",
    "  \n",
    "4. Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.  \n",
    "  \n",
    "5. Apply the linear projection layer to the decoder output, obtaining output logits.  \n",
    "  \n",
    "Above steps enable the model to process input sequences and generate output sequences based on the combined functionality of its components.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Sample Data  \n",
    "In this example, we will create a toy dataset for demonstration purposes.  \n",
    "In practice, you would use a larger dataset, preprocess the text, and  \n",
    "create vocabulary mappings for source and target languages.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "  src_vocab_size,\n",
    "  tgt_vocab_size, \n",
    "  d_model, \n",
    "  num_heads, \n",
    "  num_layers, \n",
    "  d_ff, \n",
    "  max_seq_length, \n",
    "  dropout\n",
    ")\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(\n",
    "  1, \n",
    "  src_vocab_size, \n",
    "  ( 64, max_seq_length ) # ( batch_size, seq_length )\n",
    ")\n",
    "tgt_data = torch.randint(\n",
    "  1, \n",
    "  tgt_vocab_size, \n",
    "  ( 64, max_seq_length ) # ( batch_size, seq_length )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.6810\n",
      "Epoch [2/100], Loss: 4.9840\n",
      "Epoch [3/100], Loss: 3.1237\n",
      "Epoch [4/100], Loss: 2.8086\n",
      "Epoch [5/100], Loss: 2.8458\n",
      "Epoch [6/100], Loss: 2.8390\n",
      "Epoch [7/100], Loss: 2.7423\n",
      "Epoch [8/100], Loss: 2.6211\n",
      "Epoch [9/100], Loss: 2.5546\n",
      "Epoch [10/100], Loss: 2.5156\n",
      "Epoch [11/100], Loss: 2.4820\n",
      "Epoch [12/100], Loss: 2.4541\n",
      "Epoch [13/100], Loss: 2.4177\n",
      "Epoch [14/100], Loss: 2.3797\n",
      "Epoch [15/100], Loss: 2.3394\n",
      "Epoch [16/100], Loss: 2.2943\n",
      "Epoch [17/100], Loss: 2.2600\n",
      "Epoch [18/100], Loss: 2.2182\n",
      "Epoch [19/100], Loss: 2.1800\n",
      "Epoch [20/100], Loss: 2.1487\n",
      "Epoch [21/100], Loss: 2.1097\n",
      "Epoch [22/100], Loss: 2.0806\n",
      "Epoch [23/100], Loss: 2.0432\n",
      "Epoch [24/100], Loss: 2.0150\n",
      "Epoch [25/100], Loss: 1.9828\n",
      "Epoch [26/100], Loss: 1.9473\n",
      "Epoch [27/100], Loss: 1.9167\n",
      "Epoch [28/100], Loss: 1.8840\n",
      "Epoch [29/100], Loss: 1.8502\n",
      "Epoch [30/100], Loss: 1.8136\n",
      "Epoch [31/100], Loss: 1.7913\n",
      "Epoch [32/100], Loss: 1.7556\n",
      "Epoch [33/100], Loss: 1.7266\n",
      "Epoch [34/100], Loss: 1.6955\n",
      "Epoch [35/100], Loss: 1.6681\n",
      "Epoch [36/100], Loss: 1.6321\n",
      "Epoch [37/100], Loss: 1.6029\n",
      "Epoch [38/100], Loss: 1.5802\n",
      "Epoch [39/100], Loss: 1.5444\n",
      "Epoch [40/100], Loss: 1.5164\n",
      "Epoch [41/100], Loss: 1.4905\n",
      "Epoch [42/100], Loss: 1.4583\n",
      "Epoch [43/100], Loss: 1.4270\n",
      "Epoch [44/100], Loss: 1.4010\n",
      "Epoch [45/100], Loss: 1.3787\n",
      "Epoch [46/100], Loss: 1.3512\n",
      "Epoch [47/100], Loss: 1.3257\n",
      "Epoch [48/100], Loss: 1.2939\n",
      "Epoch [49/100], Loss: 1.2678\n",
      "Epoch [50/100], Loss: 1.2449\n",
      "Epoch [51/100], Loss: 1.2198\n",
      "Epoch [52/100], Loss: 1.1875\n",
      "Epoch [53/100], Loss: 1.1707\n",
      "Epoch [54/100], Loss: 1.1460\n",
      "Epoch [55/100], Loss: 1.1167\n",
      "Epoch [56/100], Loss: 1.0996\n",
      "Epoch [57/100], Loss: 1.0738\n",
      "Epoch [58/100], Loss: 1.0475\n",
      "Epoch [59/100], Loss: 1.0304\n",
      "Epoch [60/100], Loss: 1.0052\n",
      "Epoch [61/100], Loss: 0.9866\n",
      "Epoch [62/100], Loss: 0.9625\n",
      "Epoch [63/100], Loss: 0.9432\n",
      "Epoch [64/100], Loss: 0.9203\n",
      "Epoch [65/100], Loss: 0.8941\n",
      "Epoch [66/100], Loss: 0.8799\n",
      "Epoch [67/100], Loss: 0.8616\n",
      "Epoch [68/100], Loss: 0.8392\n",
      "Epoch [69/100], Loss: 0.8139\n",
      "Epoch [70/100], Loss: 0.7973\n",
      "Epoch [71/100], Loss: 0.7863\n",
      "Epoch [72/100], Loss: 0.7665\n",
      "Epoch [73/100], Loss: 0.7475\n",
      "Epoch [74/100], Loss: 0.7255\n",
      "Epoch [75/100], Loss: 0.7153\n",
      "Epoch [76/100], Loss: 0.6935\n",
      "Epoch [77/100], Loss: 0.6814\n",
      "Epoch [78/100], Loss: 0.6627\n",
      "Epoch [79/100], Loss: 0.6499\n",
      "Epoch [80/100], Loss: 0.6339\n",
      "Epoch [81/100], Loss: 0.6206\n",
      "Epoch [82/100], Loss: 0.6055\n",
      "Epoch [83/100], Loss: 0.5910\n",
      "Epoch [84/100], Loss: 0.5728\n",
      "Epoch [85/100], Loss: 0.5640\n",
      "Epoch [86/100], Loss: 0.5505\n",
      "Epoch [87/100], Loss: 0.5382\n",
      "Epoch [88/100], Loss: 0.5253\n",
      "Epoch [89/100], Loss: 0.5094\n",
      "Epoch [90/100], Loss: 0.5000\n",
      "Epoch [91/100], Loss: 0.4908\n",
      "Epoch [92/100], Loss: 0.4766\n",
      "Epoch [93/100], Loss: 0.4644\n",
      "Epoch [94/100], Loss: 0.4552\n",
      "Epoch [95/100], Loss: 0.4469\n",
      "Epoch [96/100], Loss: 0.4361\n",
      "Epoch [97/100], Loss: 0.4243\n",
      "Epoch [98/100], Loss: 0.4167\n",
      "Epoch [99/100], Loss: 0.4055\n",
      "Epoch [100/100], Loss: 0.3984\n",
      "Epoch [101/100], Loss: 0.3895\n",
      "Epoch [102/100], Loss: 0.3793\n",
      "Epoch [103/100], Loss: 0.3717\n",
      "Epoch [104/100], Loss: 0.3610\n",
      "Epoch [105/100], Loss: 0.3510\n",
      "Epoch [106/100], Loss: 0.3485\n",
      "Epoch [107/100], Loss: 0.3394\n",
      "Epoch [108/100], Loss: 0.3303\n",
      "Epoch [109/100], Loss: 0.3241\n",
      "Epoch [110/100], Loss: 0.3172\n",
      "Epoch [111/100], Loss: 0.3089\n",
      "Epoch [112/100], Loss: 0.3023\n",
      "Epoch [113/100], Loss: 0.2965\n",
      "Epoch [114/100], Loss: 0.2904\n",
      "Epoch [115/100], Loss: 0.2840\n",
      "Epoch [116/100], Loss: 0.2806\n",
      "Epoch [117/100], Loss: 0.2719\n",
      "Epoch [118/100], Loss: 0.2675\n",
      "Epoch [119/100], Loss: 0.2597\n",
      "Epoch [120/100], Loss: 0.2560\n",
      "Epoch [121/100], Loss: 0.2508\n",
      "Epoch [122/100], Loss: 0.2442\n",
      "Epoch [123/100], Loss: 0.2436\n",
      "Epoch [124/100], Loss: 0.2362\n",
      "Epoch [125/100], Loss: 0.2326\n",
      "Epoch [126/100], Loss: 0.2281\n",
      "Epoch [127/100], Loss: 0.2235\n",
      "Epoch [128/100], Loss: 0.2175\n",
      "Epoch [129/100], Loss: 0.2134\n",
      "Epoch [130/100], Loss: 0.2094\n",
      "Epoch [131/100], Loss: 0.2048\n",
      "Epoch [132/100], Loss: 0.2025\n",
      "Epoch [133/100], Loss: 0.1977\n",
      "Epoch [134/100], Loss: 0.1933\n",
      "Epoch [135/100], Loss: 0.1900\n",
      "Epoch [136/100], Loss: 0.1865\n",
      "Epoch [137/100], Loss: 0.1832\n",
      "Epoch [138/100], Loss: 0.1804\n",
      "Epoch [139/100], Loss: 0.1768\n",
      "Epoch [140/100], Loss: 0.1734\n",
      "Epoch [141/100], Loss: 0.1695\n",
      "Epoch [142/100], Loss: 0.1669\n",
      "Epoch [143/100], Loss: 0.1626\n",
      "Epoch [144/100], Loss: 0.1618\n",
      "Epoch [145/100], Loss: 0.1588\n",
      "Epoch [146/100], Loss: 0.1550\n",
      "Epoch [147/100], Loss: 0.1516\n",
      "Epoch [148/100], Loss: 0.1502\n",
      "Epoch [149/100], Loss: 0.1474\n",
      "Epoch [150/100], Loss: 0.1445\n",
      "Epoch [151/100], Loss: 0.1418\n",
      "Epoch [152/100], Loss: 0.1409\n",
      "Epoch [153/100], Loss: 0.1368\n",
      "Epoch [154/100], Loss: 0.1350\n",
      "Epoch [155/100], Loss: 0.1330\n",
      "Epoch [156/100], Loss: 0.1309\n",
      "Epoch [157/100], Loss: 0.1286\n",
      "Epoch [158/100], Loss: 0.1257\n",
      "Epoch [159/100], Loss: 0.1237\n",
      "Epoch [160/100], Loss: 0.1222\n",
      "Epoch [161/100], Loss: 0.1192\n",
      "Epoch [162/100], Loss: 0.1176\n",
      "Epoch [163/100], Loss: 0.1164\n",
      "Epoch [164/100], Loss: 0.1139\n",
      "Epoch [165/100], Loss: 0.1124\n",
      "Epoch [166/100], Loss: 0.1107\n",
      "Epoch [167/100], Loss: 0.1091\n",
      "Epoch [168/100], Loss: 0.1073\n",
      "Epoch [169/100], Loss: 0.1049\n",
      "Epoch [170/100], Loss: 0.1036\n",
      "Epoch [171/100], Loss: 0.1020\n",
      "Epoch [172/100], Loss: 0.1011\n",
      "Epoch [173/100], Loss: 0.0998\n",
      "Epoch [174/100], Loss: 0.0971\n",
      "Epoch [175/100], Loss: 0.0957\n",
      "Epoch [176/100], Loss: 0.0942\n",
      "Epoch [177/100], Loss: 0.0923\n",
      "Epoch [178/100], Loss: 0.0923\n",
      "Epoch [179/100], Loss: 0.0894\n",
      "Epoch [180/100], Loss: 0.0891\n",
      "Epoch [181/100], Loss: 0.0872\n",
      "Epoch [182/100], Loss: 0.0859\n",
      "Epoch [183/100], Loss: 0.0852\n",
      "Epoch [184/100], Loss: 0.0835\n",
      "Epoch [185/100], Loss: 0.0824\n",
      "Epoch [186/100], Loss: 0.0813\n",
      "Epoch [187/100], Loss: 0.0803\n",
      "Epoch [188/100], Loss: 0.0785\n",
      "Epoch [189/100], Loss: 0.0772\n",
      "Epoch [190/100], Loss: 0.0767\n",
      "Epoch [191/100], Loss: 0.0757\n",
      "Epoch [192/100], Loss: 0.0749\n",
      "Epoch [193/100], Loss: 0.0737\n",
      "Epoch [194/100], Loss: 0.0722\n",
      "Epoch [195/100], Loss: 0.0713\n",
      "Epoch [196/100], Loss: 0.0705\n",
      "Epoch [197/100], Loss: 0.0701\n",
      "Epoch [198/100], Loss: 0.0684\n",
      "Epoch [199/100], Loss: 0.0673\n",
      "Epoch [200/100], Loss: 0.0665\n",
      "Epoch [201/100], Loss: 0.0654\n",
      "Epoch [202/100], Loss: 0.0646\n",
      "Epoch [203/100], Loss: 0.0637\n",
      "Epoch [204/100], Loss: 0.0624\n",
      "Epoch [205/100], Loss: 0.0621\n",
      "Epoch [206/100], Loss: 0.0608\n",
      "Epoch [207/100], Loss: 0.0602\n",
      "Epoch [208/100], Loss: 0.0591\n",
      "Epoch [209/100], Loss: 0.0582\n",
      "Epoch [210/100], Loss: 0.0579\n",
      "Epoch [211/100], Loss: 0.0578\n",
      "Epoch [212/100], Loss: 0.0563\n",
      "Epoch [213/100], Loss: 0.0551\n",
      "Epoch [214/100], Loss: 0.0548\n",
      "Epoch [215/100], Loss: 0.0541\n",
      "Epoch [216/100], Loss: 0.0532\n",
      "Epoch [217/100], Loss: 0.0528\n",
      "Epoch [218/100], Loss: 0.0522\n",
      "Epoch [219/100], Loss: 0.0510\n",
      "Epoch [220/100], Loss: 0.0505\n",
      "Epoch [221/100], Loss: 0.0500\n",
      "Epoch [222/100], Loss: 0.0489\n",
      "Epoch [223/100], Loss: 0.0486\n",
      "Epoch [224/100], Loss: 0.0475\n",
      "Epoch [225/100], Loss: 0.0469\n",
      "Epoch [226/100], Loss: 0.0469\n",
      "Epoch [227/100], Loss: 0.0464\n",
      "Epoch [228/100], Loss: 0.0454\n",
      "Epoch [229/100], Loss: 0.0453\n",
      "Epoch [230/100], Loss: 0.0441\n",
      "Epoch [231/100], Loss: 0.0436\n",
      "Epoch [232/100], Loss: 0.0430\n",
      "Epoch [233/100], Loss: 0.0430\n",
      "Epoch [234/100], Loss: 0.0422\n",
      "Epoch [235/100], Loss: 0.0416\n",
      "Epoch [236/100], Loss: 0.0409\n",
      "Epoch [237/100], Loss: 0.0404\n",
      "Epoch [238/100], Loss: 0.0403\n",
      "Epoch [239/100], Loss: 0.0396\n",
      "Epoch [240/100], Loss: 0.0388\n",
      "Epoch [241/100], Loss: 0.0385\n",
      "Epoch [242/100], Loss: 0.0381\n",
      "Epoch [243/100], Loss: 0.0377\n",
      "Epoch [244/100], Loss: 0.0374\n",
      "Epoch [245/100], Loss: 0.0366\n",
      "Epoch [246/100], Loss: 0.0362\n",
      "Epoch [247/100], Loss: 0.0358\n",
      "Epoch [248/100], Loss: 0.0352\n",
      "Epoch [249/100], Loss: 0.0348\n",
      "Epoch [250/100], Loss: 0.0345\n",
      "Epoch [251/100], Loss: 0.0339\n",
      "Epoch [252/100], Loss: 0.0337\n",
      "Epoch [253/100], Loss: 0.0332\n",
      "Epoch [254/100], Loss: 0.0325\n",
      "Epoch [255/100], Loss: 0.0325\n",
      "Epoch [256/100], Loss: 0.0320\n",
      "Epoch [257/100], Loss: 0.0318\n",
      "Epoch [258/100], Loss: 0.0314\n",
      "Epoch [259/100], Loss: 0.0308\n",
      "Epoch [260/100], Loss: 0.0305\n",
      "Epoch [261/100], Loss: 0.0301\n",
      "Epoch [262/100], Loss: 0.0294\n",
      "Epoch [263/100], Loss: 0.0293\n",
      "Epoch [264/100], Loss: 0.0292\n",
      "Epoch [265/100], Loss: 0.0285\n",
      "Epoch [266/100], Loss: 0.0283\n",
      "Epoch [267/100], Loss: 0.0278\n",
      "Epoch [268/100], Loss: 0.0277\n",
      "Epoch [269/100], Loss: 0.0273\n",
      "Epoch [270/100], Loss: 0.0268\n",
      "Epoch [271/100], Loss: 0.0265\n",
      "Epoch [272/100], Loss: 0.0261\n",
      "Epoch [273/100], Loss: 0.0262\n",
      "Epoch [274/100], Loss: 0.0258\n",
      "Epoch [275/100], Loss: 0.0254\n",
      "Epoch [276/100], Loss: 0.0248\n",
      "Epoch [277/100], Loss: 0.0246\n",
      "Epoch [278/100], Loss: 0.0245\n",
      "Epoch [279/100], Loss: 0.0239\n",
      "Epoch [280/100], Loss: 0.0240\n",
      "Epoch [281/100], Loss: 0.0236\n",
      "Epoch [282/100], Loss: 0.0233\n",
      "Epoch [283/100], Loss: 0.0230\n",
      "Epoch [284/100], Loss: 0.0227\n",
      "Epoch [285/100], Loss: 0.0223\n",
      "Epoch [286/100], Loss: 0.0221\n",
      "Epoch [287/100], Loss: 0.0221\n",
      "Epoch [288/100], Loss: 0.0217\n",
      "Epoch [289/100], Loss: 0.0214\n",
      "Epoch [290/100], Loss: 0.0210\n",
      "Epoch [291/100], Loss: 0.0211\n",
      "Epoch [292/100], Loss: 0.0207\n",
      "Epoch [293/100], Loss: 0.0205\n",
      "Epoch [294/100], Loss: 0.0201\n",
      "Epoch [295/100], Loss: 0.0200\n",
      "Epoch [296/100], Loss: 0.0196\n",
      "Epoch [297/100], Loss: 0.0196\n",
      "Epoch [298/100], Loss: 0.0193\n",
      "Epoch [299/100], Loss: 0.0192\n",
      "Epoch [300/100], Loss: 0.0188\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss( ignore_index = 0 )\n",
    "optimizer = optim.Adam( \n",
    "  transformer.parameters(), \n",
    "  lr = 0.0001, \n",
    "  betas = ( 0.9, 0.98 ),\n",
    "  eps = 1e-9\n",
    ")\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range( 300 ):\n",
    "  optimizer.zero_grad()\n",
    "  output = transformer( src_data, tgt_data[ :, :-1 ] )\n",
    "  loss = criterion(\n",
    "    output.contiguous().view( -1, tgt_vocab_size ),\n",
    "    tgt_data[ :, 1: ].contiguous().view( -1 )\n",
    "  )\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print( f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
