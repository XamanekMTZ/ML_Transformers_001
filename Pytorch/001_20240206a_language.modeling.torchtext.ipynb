{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling task  \n",
    "Assign a probablity for the likelihood of a given word (or a sequence of words)  \n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding  \n",
    "layer first, followed by a positional encoding layer to account for the order of  \n",
    "the word.  \n",
    "  \n",
    "The nn.TransformerEncoder consists of multiple layers of nn.TransformerEncoderLayer.   \n",
    "Along with the input sequence, a square attention mask is required because the self-  \n",
    "attention layers in nn.TransformerDecoder are only allowed to attend the earlier  \n",
    "positions in the sequence. For the language modeling task, any tokens on the future  \n",
    "positions should be masked. This masking, combined with fact that the output embeddings  \n",
    "are offset with later positions ensures that the predicttions for position I can depend  \n",
    "only on the known outputs at positions less than i. To produce a probability  \n",
    "distribution over output words, the output of the nn.TransformerEncoder model is passed  \n",
    "through a linear layer to output unnormalized logits. The log-softmax function isn't  \n",
    "applied here due to th elater use of CrossEntropyLoss, which requires the inputs to be  \n",
    "unnormalized logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "\n",
    "class PositionalEncoding( nn.Module ):\n",
    "\n",
    "  def __init__( self, d_model: int, dropout: float = 0.1, max_len: int = 5000 ):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout( p = dropout )\n",
    "\n",
    "    position = torch.arange( max_len ).unsqueeze( 1 )\n",
    "    div_term = torch.exp( torch.arange( 0, d_model, 2 ) * ( -math.log( 10000.0 ) / d_model ) )\n",
    "    pe = torch.zeros( max_len, 1, d_model )\n",
    "    pe[ :, 0, 0::2 ] = torch.sin( position * div_term )\n",
    "    pe[ :, 0, 1::2 ] = torch.cos( position * div_term )\n",
    "    self.register_buffer( 'pe', pe )\n",
    "\n",
    "  def forward( self, x: Tensor ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "      x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "    \"\"\"\n",
    "    x = x + self.pe[ :x.size(0) ]\n",
    "    return self.dropout( x )\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "  def __init__( self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
    "                nlayers: int, dropout: float = 0.5 ):\n",
    "    super().__init__()\n",
    "    self.model_type = 'Transformer'\n",
    "    self.pos_encoder = PositionalEncoding( d_model, dropout )\n",
    "    encoder_layers = TransformerEncoderLayer( d_model, nhead, d_hid, dropout )\n",
    "    self.transformer_encoder = TransformerEncoder( encoder_layers, nlayers )\n",
    "    self.embedding = nn.Embedding( ntoken, d_model )\n",
    "    self.d_model = d_model\n",
    "    self.linear = nn.Linear( d_model, ntoken )\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights( self ) -> None:\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_( -initrange, initrange )\n",
    "    self.linear.bias.data.zero_()\n",
    "    self.linear.weight.data.uniform_( -initrange, initrange )\n",
    "\n",
    "  def forward( self, src: Tensor, src_mask: Tensor = None ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      src: Tensor, shape ``[seq_len, batch_size]``\n",
    "      src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "    Returns:\n",
    "      output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "    \"\"\"\n",
    "    src = self.embedding( src ) * math.sqrt( self.d_model )\n",
    "    src = self.pos_encoder( src )\n",
    "    if src_mask is None:\n",
    "      \"\"\"\n",
    "      Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "      Unmasked positions are filled with float(0.0).\n",
    "      \"\"\"\n",
    "      src_mask = nn.Transformer.generate_square_subsequent_mask( len(src)).to(device)\n",
    "\n",
    "    output = self.transformer_encoder( src, src_mask )\n",
    "    output = self.linear( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PositionalEncoding( nn.Module ):\n",
    "#\n",
    "#  def __init__( self, d_model: int, dropout: float = 0.1, max_len: int = 5000 ):\n",
    "#    super().__init__()\n",
    "#    self.dropout = nn.Dropout( p = dropout )\n",
    "#\n",
    "#    position = torch.arange( max_len ).unsqueeze( 1 )\n",
    "#    div_term = torch.exp( torch.arange( 0, d_model, 2 ) * ( -math.log( 10000.0 ) / d_model ) )\n",
    "#    pe = torch.zeros( max_len, 1, d_model )\n",
    "#    pe[ :, 0, 0::2 ] = torch.sin( position * div_term )\n",
    "#    pe[ :, 0, 1::2 ] = torch.cos( position * div_term )\n",
    "#    self.register_buffer( 'pe', pe )\n",
    "#\n",
    "#  def forward( self, x: Tensor ) -> Tensor:\n",
    "#    \"\"\"\n",
    "#    Arguments: \n",
    "#      x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "#    \"\"\"\n",
    "#    x = x + self.pe[ :x.size(0) ]\n",
    "#    return self.dropout( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2( split = 'train' )\n",
    "tokenizer = get_tokenizer( 'basic_english' )\n",
    "vocab = build_vocab_from_iterator( \n",
    "  map( \n",
    "    tokenizer, train_iter \n",
    "  ), \n",
    "  specials = [   '<unk>' ] \n",
    ")\n",
    "vocab.set_default_index( vocab[ '<unk>' ] )\n",
    "\n",
    "def data_process( raw_text_iter: dataset.IterableDataset ) -> Tensor: \n",
    "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "  data = [ torch.tensor( vocab( tokenizer( item ) ), dtype = torch.long ) for item in raw_text_iter ]\n",
    "  return torch.cat( tuple( filter( lambda t: t.numel() > 0, data ) ) )\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab, \n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process( train_iter )\n",
    "val_data = data_process( val_iter )\n",
    "test_data = data_process( test_iter )\n",
    "\n",
    "def batchify( data: Tensor, bsz: int ) -> Tensor:\n",
    "  \"\"\"Divides the data into ``bsz``separate sequences, removing extra elements that wouldn't cleanly fit.\n",
    "\n",
    "  Arguments:\n",
    "    data: Tensor, shape ``[N]``\n",
    "    bsz: int, batch size\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape ``[N // bsz, bsz]``\n",
    "  \"\"\"\n",
    "  seq_len = data.size( 0 ) // bsz\n",
    "  data = data[ :seq_len * bsz ]\n",
    "  data = data.view( bsz, seq_len ).t().contiguous()\n",
    "  return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify( train_data, batch_size ) # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify( val_data, eval_batch_size )\n",
    "test_data = batchify( test_data, eval_batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch( source: Tensor, i: int ) -> Tuple[ Tensor, Tensor ]:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    source: Tensor, shape ``[seq_len, batch_size]``\n",
    "    i: int\n",
    "\n",
    "  Returns: \n",
    "    tuple (data, target), where data has shape ``[seq_len, batch_size]`` and target has shape ``[seq_len * batch_size]``\n",
    "  \"\"\"\n",
    "  seq_len = min( bptt, len( source ) -1 - i )\n",
    "  data = source[ i:i+seq_len ]\n",
    "  target = source[ i+1:i+1+seq_len ].reshape( -1 )\n",
    "  return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xamanek/.virtualenvs/Transformer001/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "d_hid = 200 # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2 # dropout probability\n",
    "model = TransformerModel( ntokens, emsize, nhead, d_hid, nlayers, dropout ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xamanek/.virtualenvs/Transformer001/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xamanek/.virtualenvs/Transformer001/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = lr )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR( optimizer, 1.0, gamma = 0.95 )\n",
    "\n",
    "def train( model: nn.Module ) -> None:\n",
    "  model.train() # Turn on the train mode\n",
    "  total_loss = 0.\n",
    "  log_interval = 200\n",
    "  start_time = time.time()\n",
    "\n",
    "  num_batches = len( train_data ) // bptt\n",
    "  for batch, i in enumerate( range( 0, train_data.size( 0 ) - 1, bptt ) ):\n",
    "    data, targets = get_batch( train_data, i )\n",
    "    output = model( data )\n",
    "    output_flat = output.view( -1, ntokens )\n",
    "    loss = criterion( output_flat, targets )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_( model.parameters(), 0.5 )\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    if batch % log_interval == 0 and batch > 0:\n",
    "      lr = scheduler.get_last_lr()[0]\n",
    "      ms_per_batch = ( time.time() - start_time ) * 1000 / log_interval\n",
    "      cur_loss = total_loss / log_interval\n",
    "      ppl = math.exp( cur_loss )\n",
    "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "            f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "      total_loss = 0\n",
    "      start_time = time.time()\n",
    "\n",
    "def evaluate( model: nn.Module, eval_data: Tensor ) -> float:\n",
    "  model.eval() # Turn on the evaluation mode\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for i in range( 0, eval_data.size(0) -1, bptt ):\n",
    "      data, targets = get_batch( eval_data, i )\n",
    "      seq_len = data.size( 0 )\n",
    "      output = model( data )\n",
    "      output_flat = output.view( -1, ntokens )\n",
    "      total_loss += seq_len * criterion( output_flat, targets ).item()\n",
    "  return total_loss / ( len( eval_data ) - 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model params path: /home/xamanek/tmp/tmpzan9qpds/best_model_params.pt\n",
      "| epoch   1 |   200/ 2928 batches | lr 2.20 | ms/batch 11.96 | loss  4.53 | ppl    93.03\n",
      "| epoch   1 |   400/ 2928 batches | lr 2.20 | ms/batch 12.05 | loss  4.55 | ppl    94.77\n",
      "| epoch   1 |   600/ 2928 batches | lr 2.20 | ms/batch 12.16 | loss  4.39 | ppl    80.67\n",
      "| epoch   1 |   800/ 2928 batches | lr 2.20 | ms/batch 12.09 | loss  4.46 | ppl    86.25\n",
      "| epoch   1 |  1000/ 2928 batches | lr 2.20 | ms/batch 12.32 | loss  4.45 | ppl    85.63\n",
      "| epoch   1 |  1200/ 2928 batches | lr 2.20 | ms/batch 11.77 | loss  4.49 | ppl    89.05\n",
      "| epoch   1 |  1400/ 2928 batches | lr 2.20 | ms/batch 11.22 | loss  4.48 | ppl    88.55\n",
      "| epoch   1 |  1600/ 2928 batches | lr 2.20 | ms/batch 11.39 | loss  4.54 | ppl    93.40\n",
      "| epoch   1 |  1800/ 2928 batches | lr 2.20 | ms/batch 11.35 | loss  4.51 | ppl    90.51\n",
      "| epoch   1 |  2000/ 2928 batches | lr 2.20 | ms/batch 11.44 | loss  4.50 | ppl    90.02\n",
      "| epoch   1 |  2200/ 2928 batches | lr 2.20 | ms/batch 11.29 | loss  4.36 | ppl    77.93\n",
      "| epoch   1 |  2400/ 2928 batches | lr 2.20 | ms/batch 11.41 | loss  4.47 | ppl    87.03\n",
      "| epoch   1 |  2600/ 2928 batches | lr 2.20 | ms/batch 11.27 | loss  4.49 | ppl    89.15\n",
      "| epoch   1 |  2800/ 2928 batches | lr 2.20 | ms/batch 11.37 | loss  4.44 | ppl    84.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 35.80s | valid loss  5.56 | valid ppl   259.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 2.09 | ms/batch 11.53 | loss  4.49 | ppl    89.53\n",
      "| epoch   2 |   400/ 2928 batches | lr 2.09 | ms/batch 11.65 | loss  4.51 | ppl    90.95\n",
      "| epoch   2 |   600/ 2928 batches | lr 2.09 | ms/batch 11.56 | loss  4.35 | ppl    77.86\n",
      "| epoch   2 |   800/ 2928 batches | lr 2.09 | ms/batch 11.66 | loss  4.42 | ppl    83.07\n",
      "| epoch   2 |  1000/ 2928 batches | lr 2.09 | ms/batch 11.57 | loss  4.41 | ppl    82.46\n",
      "| epoch   2 |  1200/ 2928 batches | lr 2.09 | ms/batch 11.58 | loss  4.45 | ppl    85.77\n",
      "| epoch   2 |  1400/ 2928 batches | lr 2.09 | ms/batch 11.39 | loss  4.44 | ppl    85.04\n",
      "| epoch   2 |  1600/ 2928 batches | lr 2.09 | ms/batch 11.48 | loss  4.50 | ppl    90.14\n",
      "| epoch   2 |  1800/ 2928 batches | lr 2.09 | ms/batch 11.43 | loss  4.47 | ppl    87.15\n",
      "| epoch   2 |  2000/ 2928 batches | lr 2.09 | ms/batch 11.40 | loss  4.46 | ppl    86.69\n",
      "| epoch   2 |  2200/ 2928 batches | lr 2.09 | ms/batch 11.53 | loss  4.32 | ppl    75.20\n",
      "| epoch   2 |  2400/ 2928 batches | lr 2.09 | ms/batch 11.54 | loss  4.43 | ppl    84.06\n",
      "| epoch   2 |  2600/ 2928 batches | lr 2.09 | ms/batch 11.80 | loss  4.46 | ppl    86.37\n",
      "| epoch   2 |  2800/ 2928 batches | lr 2.09 | ms/batch 11.76 | loss  4.40 | ppl    81.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 35.21s | valid loss  5.54 | valid ppl   255.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 1.99 | ms/batch 11.95 | loss  4.46 | ppl    86.20\n",
      "| epoch   3 |   400/ 2928 batches | lr 1.99 | ms/batch 13.07 | loss  4.48 | ppl    88.17\n",
      "| epoch   3 |   600/ 2928 batches | lr 1.99 | ms/batch 12.77 | loss  4.32 | ppl    75.50\n",
      "| epoch   3 |   800/ 2928 batches | lr 1.99 | ms/batch 12.70 | loss  4.39 | ppl    80.36\n",
      "| epoch   3 |  1000/ 2928 batches | lr 1.99 | ms/batch 12.87 | loss  4.38 | ppl    79.87\n",
      "| epoch   3 |  1200/ 2928 batches | lr 1.99 | ms/batch 12.83 | loss  4.42 | ppl    83.02\n",
      "| epoch   3 |  1400/ 2928 batches | lr 1.99 | ms/batch 12.90 | loss  4.41 | ppl    82.30\n",
      "| epoch   3 |  1600/ 2928 batches | lr 1.99 | ms/batch 12.68 | loss  4.47 | ppl    87.05\n",
      "| epoch   3 |  1800/ 2928 batches | lr 1.99 | ms/batch 12.55 | loss  4.44 | ppl    84.88\n",
      "| epoch   3 |  2000/ 2928 batches | lr 1.99 | ms/batch 11.53 | loss  4.43 | ppl    83.77\n",
      "| epoch   3 |  2200/ 2928 batches | lr 1.99 | ms/batch 11.64 | loss  4.28 | ppl    72.59\n",
      "| epoch   3 |  2400/ 2928 batches | lr 1.99 | ms/batch 11.32 | loss  4.39 | ppl    80.93\n",
      "| epoch   3 |  2600/ 2928 batches | lr 1.99 | ms/batch 11.71 | loss  4.42 | ppl    83.25\n",
      "| epoch   3 |  2800/ 2928 batches | lr 1.99 | ms/batch 11.34 | loss  4.37 | ppl    78.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 37.66s | valid loss  5.58 | valid ppl   264.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 1.89 | ms/batch 11.59 | loss  4.43 | ppl    83.88\n",
      "| epoch   4 |   400/ 2928 batches | lr 1.89 | ms/batch 11.75 | loss  4.45 | ppl    85.34\n",
      "| epoch   4 |   600/ 2928 batches | lr 1.89 | ms/batch 11.81 | loss  4.29 | ppl    72.72\n",
      "| epoch   4 |   800/ 2928 batches | lr 1.89 | ms/batch 11.50 | loss  4.35 | ppl    77.45\n",
      "| epoch   4 |  1000/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.35 | ppl    77.62\n",
      "| epoch   4 |  1200/ 2928 batches | lr 1.89 | ms/batch 11.23 | loss  4.39 | ppl    80.40\n",
      "| epoch   4 |  1400/ 2928 batches | lr 1.89 | ms/batch 11.43 | loss  4.38 | ppl    79.88\n",
      "| epoch   4 |  1600/ 2928 batches | lr 1.89 | ms/batch 11.51 | loss  4.43 | ppl    83.82\n",
      "| epoch   4 |  1800/ 2928 batches | lr 1.89 | ms/batch 11.64 | loss  4.41 | ppl    81.94\n",
      "| epoch   4 |  2000/ 2928 batches | lr 1.89 | ms/batch 11.37 | loss  4.40 | ppl    81.29\n",
      "| epoch   4 |  2200/ 2928 batches | lr 1.89 | ms/batch 11.72 | loss  4.25 | ppl    70.44\n",
      "| epoch   4 |  2400/ 2928 batches | lr 1.89 | ms/batch 11.67 | loss  4.36 | ppl    78.18\n",
      "| epoch   4 |  2600/ 2928 batches | lr 1.89 | ms/batch 11.44 | loss  4.40 | ppl    81.06\n",
      "| epoch   4 |  2800/ 2928 batches | lr 1.89 | ms/batch 11.90 | loss  4.33 | ppl    76.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 35.80s | valid loss  5.56 | valid ppl   261.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 1.79 | ms/batch 11.79 | loss  4.40 | ppl    81.55\n",
      "| epoch   5 |   400/ 2928 batches | lr 1.79 | ms/batch 11.49 | loss  4.42 | ppl    82.71\n",
      "| epoch   5 |   600/ 2928 batches | lr 1.79 | ms/batch 11.85 | loss  4.26 | ppl    71.11\n",
      "| epoch   5 |   800/ 2928 batches | lr 1.79 | ms/batch 11.61 | loss  4.33 | ppl    75.62\n",
      "| epoch   5 |  1000/ 2928 batches | lr 1.79 | ms/batch 11.85 | loss  4.32 | ppl    75.40\n",
      "| epoch   5 |  1200/ 2928 batches | lr 1.79 | ms/batch 11.65 | loss  4.36 | ppl    77.98\n",
      "| epoch   5 |  1400/ 2928 batches | lr 1.79 | ms/batch 11.54 | loss  4.35 | ppl    77.17\n",
      "| epoch   5 |  1600/ 2928 batches | lr 1.79 | ms/batch 11.68 | loss  4.41 | ppl    81.91\n",
      "| epoch   5 |  1800/ 2928 batches | lr 1.79 | ms/batch 11.63 | loss  4.38 | ppl    79.76\n",
      "| epoch   5 |  2000/ 2928 batches | lr 1.79 | ms/batch 11.95 | loss  4.37 | ppl    78.85\n",
      "| epoch   5 |  2200/ 2928 batches | lr 1.79 | ms/batch 11.57 | loss  4.23 | ppl    68.76\n",
      "| epoch   5 |  2400/ 2928 batches | lr 1.79 | ms/batch 11.53 | loss  4.33 | ppl    76.08\n",
      "| epoch   5 |  2600/ 2928 batches | lr 1.79 | ms/batch 11.61 | loss  4.36 | ppl    78.11\n",
      "| epoch   5 |  2800/ 2928 batches | lr 1.79 | ms/batch 11.69 | loss  4.30 | ppl    74.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 35.40s | valid loss  5.57 | valid ppl   262.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 1.70 | ms/batch 11.77 | loss  4.37 | ppl    78.80\n",
      "| epoch   6 |   400/ 2928 batches | lr 1.70 | ms/batch 11.98 | loss  4.39 | ppl    80.29\n",
      "| epoch   6 |   600/ 2928 batches | lr 1.70 | ms/batch 11.78 | loss  4.23 | ppl    69.06\n",
      "| epoch   6 |   800/ 2928 batches | lr 1.70 | ms/batch 12.06 | loss  4.30 | ppl    73.58\n",
      "| epoch   6 |  1000/ 2928 batches | lr 1.70 | ms/batch 11.84 | loss  4.30 | ppl    73.37\n",
      "| epoch   6 |  1200/ 2928 batches | lr 1.70 | ms/batch 11.67 | loss  4.33 | ppl    76.12\n",
      "| epoch   6 |  1400/ 2928 batches | lr 1.70 | ms/batch 11.71 | loss  4.32 | ppl    75.08\n",
      "| epoch   6 |  1600/ 2928 batches | lr 1.70 | ms/batch 11.94 | loss  4.37 | ppl    79.25\n",
      "| epoch   6 |  1800/ 2928 batches | lr 1.70 | ms/batch 11.51 | loss  4.35 | ppl    77.48\n",
      "| epoch   6 |  2000/ 2928 batches | lr 1.70 | ms/batch 11.61 | loss  4.34 | ppl    76.95\n",
      "| epoch   6 |  2200/ 2928 batches | lr 1.70 | ms/batch 11.84 | loss  4.20 | ppl    66.79\n",
      "| epoch   6 |  2400/ 2928 batches | lr 1.70 | ms/batch 11.66 | loss  4.31 | ppl    74.35\n",
      "| epoch   6 |  2600/ 2928 batches | lr 1.70 | ms/batch 11.79 | loss  4.33 | ppl    76.07\n",
      "| epoch   6 |  2800/ 2928 batches | lr 1.70 | ms/batch 11.91 | loss  4.28 | ppl    72.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 36.44s | valid loss  5.58 | valid ppl   264.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 1.62 | ms/batch 11.78 | loss  4.35 | ppl    77.13\n",
      "| epoch   7 |   400/ 2928 batches | lr 1.62 | ms/batch 11.74 | loss  4.36 | ppl    78.04\n",
      "| epoch   7 |   600/ 2928 batches | lr 1.62 | ms/batch 11.74 | loss  4.21 | ppl    67.26\n",
      "| epoch   7 |   800/ 2928 batches | lr 1.62 | ms/batch 11.96 | loss  4.28 | ppl    71.94\n",
      "| epoch   7 |  1000/ 2928 batches | lr 1.62 | ms/batch 11.53 | loss  4.27 | ppl    71.64\n",
      "| epoch   7 |  1200/ 2928 batches | lr 1.62 | ms/batch 11.87 | loss  4.30 | ppl    73.82\n",
      "| epoch   7 |  1400/ 2928 batches | lr 1.62 | ms/batch 11.60 | loss  4.30 | ppl    73.52\n",
      "| epoch   7 |  1600/ 2928 batches | lr 1.62 | ms/batch 11.85 | loss  4.35 | ppl    77.37\n",
      "| epoch   7 |  1800/ 2928 batches | lr 1.62 | ms/batch 12.11 | loss  4.33 | ppl    75.66\n",
      "| epoch   7 |  2000/ 2928 batches | lr 1.62 | ms/batch 11.97 | loss  4.31 | ppl    74.70\n",
      "| epoch   7 |  2200/ 2928 batches | lr 1.62 | ms/batch 11.67 | loss  4.17 | ppl    64.75\n",
      "| epoch   7 |  2400/ 2928 batches | lr 1.62 | ms/batch 11.54 | loss  4.28 | ppl    72.09\n",
      "| epoch   7 |  2600/ 2928 batches | lr 1.62 | ms/batch 11.84 | loss  4.31 | ppl    74.28\n",
      "| epoch   7 |  2800/ 2928 batches | lr 1.62 | ms/batch 11.90 | loss  4.25 | ppl    70.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 35.92s | valid loss  5.58 | valid ppl   264.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 1.54 | ms/batch 12.09 | loss  4.32 | ppl    74.89\n",
      "| epoch   8 |   400/ 2928 batches | lr 1.54 | ms/batch 11.99 | loss  4.33 | ppl    75.92\n",
      "| epoch   8 |   600/ 2928 batches | lr 1.54 | ms/batch 12.03 | loss  4.19 | ppl    65.70\n",
      "| epoch   8 |   800/ 2928 batches | lr 1.54 | ms/batch 11.93 | loss  4.25 | ppl    70.12\n",
      "| epoch   8 |  1000/ 2928 batches | lr 1.54 | ms/batch 11.91 | loss  4.25 | ppl    70.06\n",
      "| epoch   8 |  1200/ 2928 batches | lr 1.54 | ms/batch 11.85 | loss  4.28 | ppl    72.47\n",
      "| epoch   8 |  1400/ 2928 batches | lr 1.54 | ms/batch 11.95 | loss  4.27 | ppl    71.87\n",
      "| epoch   8 |  1600/ 2928 batches | lr 1.54 | ms/batch 11.52 | loss  4.32 | ppl    75.41\n",
      "| epoch   8 |  1800/ 2928 batches | lr 1.54 | ms/batch 12.03 | loss  4.30 | ppl    73.87\n",
      "| epoch   8 |  2000/ 2928 batches | lr 1.54 | ms/batch 11.69 | loss  4.30 | ppl    73.51\n",
      "| epoch   8 |  2200/ 2928 batches | lr 1.54 | ms/batch 11.66 | loss  4.15 | ppl    63.47\n",
      "| epoch   8 |  2400/ 2928 batches | lr 1.54 | ms/batch 12.04 | loss  4.26 | ppl    70.57\n",
      "| epoch   8 |  2600/ 2928 batches | lr 1.54 | ms/batch 12.15 | loss  4.28 | ppl    72.16\n",
      "| epoch   8 |  2800/ 2928 batches | lr 1.54 | ms/batch 12.20 | loss  4.23 | ppl    68.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 36.82s | valid loss  5.58 | valid ppl   264.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 1.46 | ms/batch 12.13 | loss  4.30 | ppl    73.50\n",
      "| epoch   9 |   400/ 2928 batches | lr 1.46 | ms/batch 11.92 | loss  4.31 | ppl    74.77\n",
      "| epoch   9 |   600/ 2928 batches | lr 1.46 | ms/batch 11.75 | loss  4.16 | ppl    64.03\n",
      "| epoch   9 |   800/ 2928 batches | lr 1.46 | ms/batch 11.77 | loss  4.23 | ppl    68.49\n",
      "| epoch   9 |  1000/ 2928 batches | lr 1.46 | ms/batch 12.00 | loss  4.23 | ppl    68.70\n",
      "| epoch   9 |  1200/ 2928 batches | lr 1.46 | ms/batch 11.85 | loss  4.26 | ppl    70.66\n",
      "| epoch   9 |  1400/ 2928 batches | lr 1.46 | ms/batch 12.07 | loss  4.25 | ppl    70.11\n",
      "| epoch   9 |  1600/ 2928 batches | lr 1.46 | ms/batch 12.00 | loss  4.30 | ppl    73.89\n",
      "| epoch   9 |  1800/ 2928 batches | lr 1.46 | ms/batch 12.04 | loss  4.28 | ppl    72.41\n",
      "| epoch   9 |  2000/ 2928 batches | lr 1.46 | ms/batch 11.95 | loss  4.27 | ppl    71.66\n",
      "| epoch   9 |  2200/ 2928 batches | lr 1.46 | ms/batch 12.08 | loss  4.12 | ppl    61.85\n",
      "| epoch   9 |  2400/ 2928 batches | lr 1.46 | ms/batch 12.04 | loss  4.23 | ppl    68.93\n",
      "| epoch   9 |  2600/ 2928 batches | lr 1.46 | ms/batch 11.87 | loss  4.26 | ppl    70.75\n",
      "| epoch   9 |  2800/ 2928 batches | lr 1.46 | ms/batch 11.94 | loss  4.21 | ppl    67.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 36.83s | valid loss  5.59 | valid ppl   267.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 1.39 | ms/batch 12.04 | loss  4.28 | ppl    72.10\n",
      "| epoch  10 |   400/ 2928 batches | lr 1.39 | ms/batch 11.95 | loss  4.29 | ppl    72.83\n",
      "| epoch  10 |   600/ 2928 batches | lr 1.39 | ms/batch 12.31 | loss  4.14 | ppl    62.86\n",
      "| epoch  10 |   800/ 2928 batches | lr 1.39 | ms/batch 11.97 | loss  4.21 | ppl    67.21\n",
      "| epoch  10 |  1000/ 2928 batches | lr 1.39 | ms/batch 12.16 | loss  4.21 | ppl    67.16\n",
      "| epoch  10 |  1200/ 2928 batches | lr 1.39 | ms/batch 11.78 | loss  4.24 | ppl    69.14\n",
      "| epoch  10 |  1400/ 2928 batches | lr 1.39 | ms/batch 11.91 | loss  4.23 | ppl    68.77\n",
      "| epoch  10 |  1600/ 2928 batches | lr 1.39 | ms/batch 12.05 | loss  4.28 | ppl    72.33\n",
      "| epoch  10 |  1800/ 2928 batches | lr 1.39 | ms/batch 12.00 | loss  4.26 | ppl    70.93\n",
      "| epoch  10 |  2000/ 2928 batches | lr 1.39 | ms/batch 12.10 | loss  4.25 | ppl    70.21\n",
      "| epoch  10 |  2200/ 2928 batches | lr 1.39 | ms/batch 11.99 | loss  4.11 | ppl    60.74\n",
      "| epoch  10 |  2400/ 2928 batches | lr 1.39 | ms/batch 11.79 | loss  4.21 | ppl    67.36\n",
      "| epoch  10 |  2600/ 2928 batches | lr 1.39 | ms/batch 12.08 | loss  4.23 | ppl    69.06\n",
      "| epoch  10 |  2800/ 2928 batches | lr 1.39 | ms/batch 11.83 | loss  4.19 | ppl    65.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 37.01s | valid loss  5.58 | valid ppl   266.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2928 batches | lr 1.32 | ms/batch 12.38 | loss  4.25 | ppl    70.27\n",
      "| epoch  11 |   400/ 2928 batches | lr 1.32 | ms/batch 11.76 | loss  4.27 | ppl    71.63\n",
      "| epoch  11 |   600/ 2928 batches | lr 1.32 | ms/batch 12.27 | loss  4.12 | ppl    61.74\n",
      "| epoch  11 |   800/ 2928 batches | lr 1.32 | ms/batch 12.06 | loss  4.19 | ppl    66.02\n",
      "| epoch  11 |  1000/ 2928 batches | lr 1.32 | ms/batch 11.97 | loss  4.19 | ppl    65.98\n",
      "| epoch  11 |  1200/ 2928 batches | lr 1.32 | ms/batch 12.26 | loss  4.22 | ppl    68.35\n",
      "| epoch  11 |  1400/ 2928 batches | lr 1.32 | ms/batch 11.95 | loss  4.21 | ppl    67.45\n",
      "| epoch  11 |  1600/ 2928 batches | lr 1.32 | ms/batch 12.14 | loss  4.26 | ppl    70.76\n",
      "| epoch  11 |  1800/ 2928 batches | lr 1.32 | ms/batch 12.14 | loss  4.24 | ppl    69.66\n",
      "| epoch  11 |  2000/ 2928 batches | lr 1.32 | ms/batch 11.95 | loss  4.23 | ppl    68.74\n",
      "| epoch  11 |  2200/ 2928 batches | lr 1.32 | ms/batch 11.84 | loss  4.09 | ppl    59.50\n",
      "| epoch  11 |  2400/ 2928 batches | lr 1.32 | ms/batch 12.18 | loss  4.19 | ppl    66.03\n",
      "| epoch  11 |  2600/ 2928 batches | lr 1.32 | ms/batch 12.09 | loss  4.22 | ppl    67.82\n",
      "| epoch  11 |  2800/ 2928 batches | lr 1.32 | ms/batch 12.29 | loss  4.17 | ppl    64.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 37.32s | valid loss  5.58 | valid ppl   263.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 2928 batches | lr 1.25 | ms/batch 12.95 | loss  4.24 | ppl    69.18\n",
      "| epoch  12 |   400/ 2928 batches | lr 1.25 | ms/batch 12.04 | loss  4.25 | ppl    69.97\n",
      "| epoch  12 |   600/ 2928 batches | lr 1.25 | ms/batch 12.46 | loss  4.10 | ppl    60.37\n",
      "| epoch  12 |   800/ 2928 batches | lr 1.25 | ms/batch 11.87 | loss  4.17 | ppl    64.42\n",
      "| epoch  12 |  1000/ 2928 batches | lr 1.25 | ms/batch 11.70 | loss  4.17 | ppl    64.73\n",
      "| epoch  12 |  1200/ 2928 batches | lr 1.25 | ms/batch 11.97 | loss  4.20 | ppl    66.79\n",
      "| epoch  12 |  1400/ 2928 batches | lr 1.25 | ms/batch 12.02 | loss  4.19 | ppl    66.19\n",
      "| epoch  12 |  1600/ 2928 batches | lr 1.25 | ms/batch 12.02 | loss  4.24 | ppl    69.48\n",
      "| epoch  12 |  1800/ 2928 batches | lr 1.25 | ms/batch 11.89 | loss  4.22 | ppl    68.26\n",
      "| epoch  12 |  2000/ 2928 batches | lr 1.25 | ms/batch 11.93 | loss  4.21 | ppl    67.69\n",
      "| epoch  12 |  2200/ 2928 batches | lr 1.25 | ms/batch 12.08 | loss  4.07 | ppl    58.42\n",
      "| epoch  12 |  2400/ 2928 batches | lr 1.25 | ms/batch 12.17 | loss  4.17 | ppl    64.98\n",
      "| epoch  12 |  2600/ 2928 batches | lr 1.25 | ms/batch 12.05 | loss  4.20 | ppl    66.61\n",
      "| epoch  12 |  2800/ 2928 batches | lr 1.25 | ms/batch 12.21 | loss  4.15 | ppl    63.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 37.29s | valid loss  5.60 | valid ppl   270.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2928 batches | lr 1.19 | ms/batch 13.50 | loss  4.22 | ppl    67.73\n",
      "| epoch  13 |   400/ 2928 batches | lr 1.19 | ms/batch 13.46 | loss  4.23 | ppl    68.65\n",
      "| epoch  13 |   600/ 2928 batches | lr 1.19 | ms/batch 13.32 | loss  4.08 | ppl    59.27\n",
      "| epoch  13 |   800/ 2928 batches | lr 1.19 | ms/batch 13.49 | loss  4.15 | ppl    63.31\n",
      "| epoch  13 |  1000/ 2928 batches | lr 1.19 | ms/batch 13.44 | loss  4.15 | ppl    63.47\n",
      "| epoch  13 |  1200/ 2928 batches | lr 1.19 | ms/batch 13.16 | loss  4.18 | ppl    65.23\n",
      "| epoch  13 |  1400/ 2928 batches | lr 1.19 | ms/batch 13.47 | loss  4.17 | ppl    64.66\n",
      "| epoch  13 |  1600/ 2928 batches | lr 1.19 | ms/batch 13.15 | loss  4.22 | ppl    67.99\n",
      "| epoch  13 |  1800/ 2928 batches | lr 1.19 | ms/batch 12.40 | loss  4.20 | ppl    66.91\n",
      "| epoch  13 |  2000/ 2928 batches | lr 1.19 | ms/batch 13.23 | loss  4.20 | ppl    66.47\n",
      "| epoch  13 |  2200/ 2928 batches | lr 1.19 | ms/batch 12.47 | loss  4.04 | ppl    57.06\n",
      "| epoch  13 |  2400/ 2928 batches | lr 1.19 | ms/batch 11.71 | loss  4.15 | ppl    63.63\n",
      "| epoch  13 |  2600/ 2928 batches | lr 1.19 | ms/batch 11.13 | loss  4.18 | ppl    65.14\n",
      "| epoch  13 |  2800/ 2928 batches | lr 1.19 | ms/batch 11.13 | loss  4.13 | ppl    62.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 38.43s | valid loss  5.60 | valid ppl   270.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2928 batches | lr 1.13 | ms/batch 11.02 | loss  4.20 | ppl    66.79\n",
      "| epoch  14 |   400/ 2928 batches | lr 1.13 | ms/batch 10.97 | loss  4.21 | ppl    67.43\n",
      "| epoch  14 |   600/ 2928 batches | lr 1.13 | ms/batch 11.19 | loss  4.07 | ppl    58.52\n",
      "| epoch  14 |   800/ 2928 batches | lr 1.13 | ms/batch 11.19 | loss  4.13 | ppl    62.36\n",
      "| epoch  14 |  1000/ 2928 batches | lr 1.13 | ms/batch 11.27 | loss  4.14 | ppl    62.49\n",
      "| epoch  14 |  1200/ 2928 batches | lr 1.13 | ms/batch 11.33 | loss  4.17 | ppl    64.53\n",
      "| epoch  14 |  1400/ 2928 batches | lr 1.13 | ms/batch 10.63 | loss  4.16 | ppl    63.79\n",
      "| epoch  14 |  1600/ 2928 batches | lr 1.13 | ms/batch 11.14 | loss  4.21 | ppl    67.32\n",
      "| epoch  14 |  1800/ 2928 batches | lr 1.13 | ms/batch 11.27 | loss  4.19 | ppl    65.96\n",
      "| epoch  14 |  2000/ 2928 batches | lr 1.13 | ms/batch 11.13 | loss  4.18 | ppl    65.10\n",
      "| epoch  14 |  2200/ 2928 batches | lr 1.13 | ms/batch 11.12 | loss  4.03 | ppl    56.43\n",
      "| epoch  14 |  2400/ 2928 batches | lr 1.13 | ms/batch 10.84 | loss  4.14 | ppl    62.56\n",
      "| epoch  14 |  2600/ 2928 batches | lr 1.13 | ms/batch 10.94 | loss  4.16 | ppl    63.97\n",
      "| epoch  14 |  2800/ 2928 batches | lr 1.13 | ms/batch 11.02 | loss  4.12 | ppl    61.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 34.26s | valid loss  5.60 | valid ppl   269.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2928 batches | lr 1.07 | ms/batch 10.94 | loss  4.19 | ppl    65.77\n",
      "| epoch  15 |   400/ 2928 batches | lr 1.07 | ms/batch 11.11 | loss  4.20 | ppl    66.57\n",
      "| epoch  15 |   600/ 2928 batches | lr 1.07 | ms/batch 10.77 | loss  4.05 | ppl    57.59\n",
      "| epoch  15 |   800/ 2928 batches | lr 1.07 | ms/batch 11.02 | loss  4.12 | ppl    61.49\n",
      "| epoch  15 |  1000/ 2928 batches | lr 1.07 | ms/batch 10.86 | loss  4.12 | ppl    61.47\n",
      "| epoch  15 |  1200/ 2928 batches | lr 1.07 | ms/batch 11.20 | loss  4.15 | ppl    63.38\n",
      "| epoch  15 |  1400/ 2928 batches | lr 1.07 | ms/batch 11.16 | loss  4.14 | ppl    62.54\n",
      "| epoch  15 |  1600/ 2928 batches | lr 1.07 | ms/batch 11.03 | loss  4.19 | ppl    66.17\n",
      "| epoch  15 |  1800/ 2928 batches | lr 1.07 | ms/batch 11.22 | loss  4.18 | ppl    65.06\n",
      "| epoch  15 |  2000/ 2928 batches | lr 1.07 | ms/batch 11.20 | loss  4.16 | ppl    64.19\n",
      "| epoch  15 |  2200/ 2928 batches | lr 1.07 | ms/batch 11.04 | loss  4.02 | ppl    55.57\n",
      "| epoch  15 |  2400/ 2928 batches | lr 1.07 | ms/batch 11.53 | loss  4.12 | ppl    61.60\n",
      "| epoch  15 |  2600/ 2928 batches | lr 1.07 | ms/batch 10.99 | loss  4.14 | ppl    63.02\n",
      "| epoch  15 |  2800/ 2928 batches | lr 1.07 | ms/batch 10.60 | loss  4.10 | ppl    60.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 33.53s | valid loss  5.60 | valid ppl   270.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2928 batches | lr 1.02 | ms/batch 12.46 | loss  4.17 | ppl    64.85\n",
      "| epoch  16 |   400/ 2928 batches | lr 1.02 | ms/batch 12.36 | loss  4.18 | ppl    65.40\n",
      "| epoch  16 |   600/ 2928 batches | lr 1.02 | ms/batch 11.96 | loss  4.04 | ppl    56.89\n",
      "| epoch  16 |   800/ 2928 batches | lr 1.02 | ms/batch 11.78 | loss  4.10 | ppl    60.46\n",
      "| epoch  16 |  1000/ 2928 batches | lr 1.02 | ms/batch 11.94 | loss  4.11 | ppl    60.91\n",
      "| epoch  16 |  1200/ 2928 batches | lr 1.02 | ms/batch 11.57 | loss  4.14 | ppl    62.51\n",
      "| epoch  16 |  1400/ 2928 batches | lr 1.02 | ms/batch 11.39 | loss  4.12 | ppl    61.68\n",
      "| epoch  16 |  1600/ 2928 batches | lr 1.02 | ms/batch 10.90 | loss  4.18 | ppl    65.12\n",
      "| epoch  16 |  1800/ 2928 batches | lr 1.02 | ms/batch 11.08 | loss  4.16 | ppl    64.18\n",
      "| epoch  16 |  2000/ 2928 batches | lr 1.02 | ms/batch 10.63 | loss  4.15 | ppl    63.69\n",
      "| epoch  16 |  2200/ 2928 batches | lr 1.02 | ms/batch 10.92 | loss  4.00 | ppl    54.42\n",
      "| epoch  16 |  2400/ 2928 batches | lr 1.02 | ms/batch 11.02 | loss  4.11 | ppl    60.74\n",
      "| epoch  16 |  2600/ 2928 batches | lr 1.02 | ms/batch 10.75 | loss  4.13 | ppl    62.35\n",
      "| epoch  16 |  2800/ 2928 batches | lr 1.02 | ms/batch 11.41 | loss  4.09 | ppl    59.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 35.46s | valid loss  5.61 | valid ppl   273.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2928 batches | lr 0.97 | ms/batch 11.94 | loss  4.16 | ppl    63.89\n",
      "| epoch  17 |   400/ 2928 batches | lr 0.97 | ms/batch 11.25 | loss  4.17 | ppl    64.59\n",
      "| epoch  17 |   600/ 2928 batches | lr 0.97 | ms/batch 12.18 | loss  4.02 | ppl    55.94\n",
      "| epoch  17 |   800/ 2928 batches | lr 0.97 | ms/batch 11.71 | loss  4.09 | ppl    59.70\n",
      "| epoch  17 |  1000/ 2928 batches | lr 0.97 | ms/batch 12.61 | loss  4.09 | ppl    59.94\n",
      "| epoch  17 |  1200/ 2928 batches | lr 0.97 | ms/batch 11.49 | loss  4.12 | ppl    61.58\n",
      "| epoch  17 |  1400/ 2928 batches | lr 0.97 | ms/batch 11.35 | loss  4.11 | ppl    60.73\n",
      "| epoch  17 |  1600/ 2928 batches | lr 0.97 | ms/batch 11.02 | loss  4.16 | ppl    64.11\n",
      "| epoch  17 |  1800/ 2928 batches | lr 0.97 | ms/batch 11.22 | loss  4.15 | ppl    63.23\n",
      "| epoch  17 |  2000/ 2928 batches | lr 0.97 | ms/batch 10.99 | loss  4.14 | ppl    62.52\n",
      "| epoch  17 |  2200/ 2928 batches | lr 0.97 | ms/batch 10.95 | loss  3.99 | ppl    54.13\n",
      "| epoch  17 |  2400/ 2928 batches | lr 0.97 | ms/batch 11.26 | loss  4.09 | ppl    59.70\n",
      "| epoch  17 |  2600/ 2928 batches | lr 0.97 | ms/batch 10.85 | loss  4.11 | ppl    61.22\n",
      "| epoch  17 |  2800/ 2928 batches | lr 0.97 | ms/batch 10.82 | loss  4.08 | ppl    58.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 35.12s | valid loss  5.62 | valid ppl   274.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2928 batches | lr 0.92 | ms/batch 10.95 | loss  4.14 | ppl    62.84\n",
      "| epoch  18 |   400/ 2928 batches | lr 0.92 | ms/batch 11.12 | loss  4.16 | ppl    63.96\n",
      "| epoch  18 |   600/ 2928 batches | lr 0.92 | ms/batch 11.15 | loss  4.01 | ppl    55.15\n",
      "| epoch  18 |   800/ 2928 batches | lr 0.92 | ms/batch 10.78 | loss  4.08 | ppl    58.97\n",
      "| epoch  18 |  1000/ 2928 batches | lr 0.92 | ms/batch 11.12 | loss  4.09 | ppl    59.58\n",
      "| epoch  18 |  1200/ 2928 batches | lr 0.92 | ms/batch 11.25 | loss  4.11 | ppl    61.14\n",
      "| epoch  18 |  1400/ 2928 batches | lr 0.92 | ms/batch 11.03 | loss  4.10 | ppl    60.04\n",
      "| epoch  18 |  1600/ 2928 batches | lr 0.92 | ms/batch 11.32 | loss  4.15 | ppl    63.34\n",
      "| epoch  18 |  1800/ 2928 batches | lr 0.92 | ms/batch 11.10 | loss  4.14 | ppl    62.58\n",
      "| epoch  18 |  2000/ 2928 batches | lr 0.92 | ms/batch 10.87 | loss  4.12 | ppl    61.66\n",
      "| epoch  18 |  2200/ 2928 batches | lr 0.92 | ms/batch 11.10 | loss  3.98 | ppl    53.59\n",
      "| epoch  18 |  2400/ 2928 batches | lr 0.92 | ms/batch 11.24 | loss  4.08 | ppl    58.95\n",
      "| epoch  18 |  2600/ 2928 batches | lr 0.92 | ms/batch 11.15 | loss  4.10 | ppl    60.59\n",
      "| epoch  18 |  2800/ 2928 batches | lr 0.92 | ms/batch 11.03 | loss  4.06 | ppl    58.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 33.66s | valid loss  5.62 | valid ppl   275.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2928 batches | lr 0.87 | ms/batch 11.23 | loss  4.13 | ppl    62.34\n",
      "| epoch  19 |   400/ 2928 batches | lr 0.87 | ms/batch 11.15 | loss  4.14 | ppl    63.01\n",
      "| epoch  19 |   600/ 2928 batches | lr 0.87 | ms/batch 11.18 | loss  4.00 | ppl    54.72\n",
      "| epoch  19 |   800/ 2928 batches | lr 0.87 | ms/batch 11.16 | loss  4.06 | ppl    58.21\n",
      "| epoch  19 |  1000/ 2928 batches | lr 0.87 | ms/batch 11.22 | loss  4.07 | ppl    58.33\n",
      "| epoch  19 |  1200/ 2928 batches | lr 0.87 | ms/batch 11.09 | loss  4.10 | ppl    60.32\n",
      "| epoch  19 |  1400/ 2928 batches | lr 0.87 | ms/batch 10.99 | loss  4.08 | ppl    59.09\n",
      "| epoch  19 |  1600/ 2928 batches | lr 0.87 | ms/batch 11.13 | loss  4.13 | ppl    62.47\n",
      "| epoch  19 |  1800/ 2928 batches | lr 0.87 | ms/batch 11.04 | loss  4.13 | ppl    61.95\n",
      "| epoch  19 |  2000/ 2928 batches | lr 0.87 | ms/batch 11.24 | loss  4.11 | ppl    60.88\n",
      "| epoch  19 |  2200/ 2928 batches | lr 0.87 | ms/batch 11.27 | loss  3.97 | ppl    52.77\n",
      "| epoch  19 |  2400/ 2928 batches | lr 0.87 | ms/batch 10.86 | loss  4.07 | ppl    58.31\n",
      "| epoch  19 |  2600/ 2928 batches | lr 0.87 | ms/batch 11.17 | loss  4.09 | ppl    59.67\n",
      "| epoch  19 |  2800/ 2928 batches | lr 0.87 | ms/batch 11.16 | loss  4.05 | ppl    57.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 34.44s | valid loss  5.60 | valid ppl   271.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2928 batches | lr 0.83 | ms/batch 10.77 | loss  4.12 | ppl    61.64\n",
      "| epoch  20 |   400/ 2928 batches | lr 0.83 | ms/batch 11.23 | loss  4.13 | ppl    62.00\n",
      "| epoch  20 |   600/ 2928 batches | lr 0.83 | ms/batch 11.04 | loss  3.99 | ppl    53.87\n",
      "| epoch  20 |   800/ 2928 batches | lr 0.83 | ms/batch 11.35 | loss  4.06 | ppl    57.77\n",
      "| epoch  20 |  1000/ 2928 batches | lr 0.83 | ms/batch 11.23 | loss  4.06 | ppl    57.95\n",
      "| epoch  20 |  1200/ 2928 batches | lr 0.83 | ms/batch 10.97 | loss  4.09 | ppl    59.76\n",
      "| epoch  20 |  1400/ 2928 batches | lr 0.83 | ms/batch 11.16 | loss  4.07 | ppl    58.65\n",
      "| epoch  20 |  1600/ 2928 batches | lr 0.83 | ms/batch 10.79 | loss  4.12 | ppl    61.82\n",
      "| epoch  20 |  1800/ 2928 batches | lr 0.83 | ms/batch 11.03 | loss  4.11 | ppl    61.00\n",
      "| epoch  20 |  2000/ 2928 batches | lr 0.83 | ms/batch 11.30 | loss  4.10 | ppl    60.19\n",
      "| epoch  20 |  2200/ 2928 batches | lr 0.83 | ms/batch 10.87 | loss  3.95 | ppl    52.09\n",
      "| epoch  20 |  2400/ 2928 batches | lr 0.83 | ms/batch 10.95 | loss  4.06 | ppl    57.96\n",
      "| epoch  20 |  2600/ 2928 batches | lr 0.83 | ms/batch 10.71 | loss  4.08 | ppl    58.93\n",
      "| epoch  20 |  2800/ 2928 batches | lr 0.83 | ms/batch 10.94 | loss  4.04 | ppl    56.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 34.10s | valid loss  5.62 | valid ppl   275.78\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "\n",
    "temp_dir_base = \"/home/xamanek/tmp\"\n",
    "\n",
    "with TemporaryDirectory(dir=temp_dir_base) as tempdir:\n",
    "\n",
    "#with TemporaryDirectory() as tempdir:\n",
    "  best_model_params_path = os.path.join( tempdir, 'best_model_params.pt' )\n",
    "\n",
    "  print( f\"Best model params path: {best_model_params_path}\")\n",
    "  for epoch in range( 1, epochs + 1 ):\n",
    "    epoch_start_time = time.time()\n",
    "    train( model )\n",
    "    val_loss = evaluate( model, val_data )\n",
    "    val_ppl = math.exp( val_loss )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print( '-' * 89 )\n",
    "    print( f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}' )\n",
    "    print( '-' * 89 )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "      best_val_loss = val_loss\n",
    "      torch.save( model.state_dict(), best_model_params_path )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "  model.load_state_dict( torch.load( best_model_params_path ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.46 | test ppl   235.72\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate( model, test_data )\n",
    "test_ppl = math.exp( test_loss )\n",
    "print( '=' * 89 )\n",
    "print( f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}' )\n",
    "print( '=' * 89 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
