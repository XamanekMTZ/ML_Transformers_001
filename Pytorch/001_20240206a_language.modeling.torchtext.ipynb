{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling task  \n",
    "Assign a probablity for the likelihood of a given word (or a sequence of words)  \n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding  \n",
    "layer first, followed by a positional encoding layer to account for the order of  \n",
    "the word.  \n",
    "  \n",
    "The nn.TransformerEncoder consists of multiple layers of nn.TransformerEncoderLayer.   \n",
    "Along with the input sequence, a square attention mask is required because the self-  \n",
    "attention layers in nn.TransformerDecoder are only allowed to attend the earlier  \n",
    "positions in the sequence. For the language modeling task, any tokens on the future  \n",
    "positions should be masked. This masking, combined with fact that the output embeddings  \n",
    "are offset with later positions ensures that the predicttions for position I can depend  \n",
    "only on the known outputs at positions less than i. To produce a probability  \n",
    "distribution over output words, the output of the nn.TransformerEncoder model is passed  \n",
    "through a linear layer to output unnormalized logits. The log-softmax function isn't  \n",
    "applied here due to th elater use of CrossEntropyLoss, which requires the inputs to be  \n",
    "unnormalized logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "\n",
    "class PositionalEncoding( nn.Module ):\n",
    "\n",
    "  def __init__( self, d_model: int, dropout: float = 0.1, max_len: int = 5000 ):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout( p = dropout )\n",
    "\n",
    "    position = torch.arange( max_len ).unsqueeze( 1 )\n",
    "    div_term = torch.exp( torch.arange( 0, d_model, 2 ) * ( -math.log( 10000.0 ) / d_model ) )\n",
    "    pe = torch.zeros( max_len, 1, d_model )\n",
    "    pe[ :, 0, 0::2 ] = torch.sin( position * div_term )\n",
    "    pe[ :, 0, 1::2 ] = torch.cos( position * div_term )\n",
    "    self.register_buffer( 'pe', pe )\n",
    "\n",
    "  def forward( self, x: Tensor ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "      x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "    \"\"\"\n",
    "    x = x + self.pe[ :x.size(0) ]\n",
    "    return self.dropout( x )\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "  def __init__( self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
    "                nlayers: int, dropout: float = 0.5 ):\n",
    "    super().__init__()\n",
    "    self.model_type = 'Transformer'\n",
    "    self.pos_encoder = PositionalEncoding( d_model, dropout )\n",
    "    encoder_layers = TransformerEncoderLayer( d_model, nhead, d_hid, dropout )\n",
    "    self.transformer_encoder = TransformerEncoder( encoder_layers, nlayers )\n",
    "    self.embedding = nn.Embedding( ntoken, d_model )\n",
    "    self.d_model = d_model\n",
    "    self.linear = nn.Linear( d_model, ntoken )\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights( self ) -> None:\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_( -initrange, initrange )\n",
    "    self.linear.bias.data.zero_()\n",
    "    self.linear.weight.data.uniform_( -initrange, initrange )\n",
    "\n",
    "  def forward( self, src: Tensor, src_mask: Tensor = None ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      src: Tensor, shape ``[seq_len, batch_size]``\n",
    "      src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "    Returns:\n",
    "      output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "    \"\"\"\n",
    "    src = self.embedding( src ) * math.sqrt( self.d_model )\n",
    "    src = self.pos_encoder( src )\n",
    "    if src_mask is None:\n",
    "      \"\"\"\n",
    "      Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "      Unmasked positions are filled with float(0.0).\n",
    "      \"\"\"\n",
    "      src_mask = nn.Transformer.generate_square_subsequent_mask( len(src)).to(device)\n",
    "\n",
    "    output = self.transformer_encoder( src, src_mask )\n",
    "    output = self.linear( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PositionalEncoding( nn.Module ):\n",
    "#\n",
    "#  def __init__( self, d_model: int, dropout: float = 0.1, max_len: int = 5000 ):\n",
    "#    super().__init__()\n",
    "#    self.dropout = nn.Dropout( p = dropout )\n",
    "#\n",
    "#    position = torch.arange( max_len ).unsqueeze( 1 )\n",
    "#    div_term = torch.exp( torch.arange( 0, d_model, 2 ) * ( -math.log( 10000.0 ) / d_model ) )\n",
    "#    pe = torch.zeros( max_len, 1, d_model )\n",
    "#    pe[ :, 0, 0::2 ] = torch.sin( position * div_term )\n",
    "#    pe[ :, 0, 1::2 ] = torch.cos( position * div_term )\n",
    "#    self.register_buffer( 'pe', pe )\n",
    "#\n",
    "#  def forward( self, x: Tensor ) -> Tensor:\n",
    "#    \"\"\"\n",
    "#    Arguments: \n",
    "#      x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "#    \"\"\"\n",
    "#    x = x + self.pe[ :x.size(0) ]\n",
    "#    return self.dropout( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2( split = 'train' )\n",
    "tokenizer = get_tokenizer( 'basic_english' )\n",
    "vocab = build_vocab_from_iterator( \n",
    "  map( \n",
    "    tokenizer, train_iter \n",
    "  ), \n",
    "  specials = [   '<unk>' ] \n",
    ")\n",
    "vocab.set_default_index( vocab[ '<unk>' ] )\n",
    "\n",
    "def data_process( raw_text_iter: dataset.IterableDataset ) -> Tensor: \n",
    "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "  data = [ torch.tensor( vocab( tokenizer( item ) ), dtype = torch.long ) for item in raw_text_iter ]\n",
    "  return torch.cat( tuple( filter( lambda t: t.numel() > 0, data ) ) )\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab, \n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process( train_iter )\n",
    "val_data = data_process( val_iter )\n",
    "test_data = data_process( test_iter )\n",
    "\n",
    "def batchify( data: Tensor, bsz: int ) -> Tensor:\n",
    "  \"\"\"Divides the data into ``bsz``separate sequences, removing extra elements that wouldn't cleanly fit.\n",
    "\n",
    "  Arguments:\n",
    "    data: Tensor, shape ``[N]``\n",
    "    bsz: int, batch size\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape ``[N // bsz, bsz]``\n",
    "  \"\"\"\n",
    "  seq_len = data.size( 0 ) // bsz\n",
    "  data = data[ :seq_len * bsz ]\n",
    "  data = data.view( bsz, seq_len ).t().contiguous()\n",
    "  return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify( train_data, batch_size ) # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify( val_data, eval_batch_size )\n",
    "test_data = batchify( test_data, eval_batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch( source: Tensor, i: int ) -> Tuple[ Tensor, Tensor ]:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    source: Tensor, shape ``[seq_len, batch_size]``\n",
    "    i: int\n",
    "\n",
    "  Returns: \n",
    "    tuple (data, target), where data has shape ``[seq_len, batch_size]`` and target has shape ``[seq_len * batch_size]``\n",
    "  \"\"\"\n",
    "  seq_len = min( bptt, len( source ) -1 - i )\n",
    "  data = source[ i:i+seq_len ]\n",
    "  target = source[ i+1:i+1+seq_len ].reshape( -1 )\n",
    "  return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xamanek/.virtual_envs/TransformersML/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "d_hid = 200 # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2 # dropout probability\n",
    "model = TransformerModel( ntokens, emsize, nhead, d_hid, nlayers, dropout ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = lr )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR( optimizer, 1.0, gamma = 0.95 )\n",
    "\n",
    "def train( model: nn.Module ) -> None:\n",
    "  model.train() # Turn on the train mode\n",
    "  total_loss = 0.\n",
    "  log_interval = 200\n",
    "  start_time = time.time()\n",
    "\n",
    "  num_batches = len( train_data ) // bptt\n",
    "  for batch, i in enumerate( range( 0, train_data.size( 0 ) - 1, bptt ) ):\n",
    "    data, targets = get_batch( train_data, i )\n",
    "    output = model( data )\n",
    "    output_flat = output.view( -1, ntokens )\n",
    "    loss = criterion( output_flat, targets )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_( model.parameters(), 0.5 )\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    if batch % log_interval == 0 and batch > 0:\n",
    "      lr = scheduler.get_last_lr()[0]\n",
    "      ms_per_batch = ( time.time() - start_time ) * 1000 / log_interval\n",
    "      cur_loss = total_loss / log_interval\n",
    "      ppl = math.exp( cur_loss )\n",
    "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "            f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "      total_loss = 0\n",
    "      start_time = time.time()\n",
    "\n",
    "def evaluate( model: nn.Module, eval_data: Tensor ) -> float:\n",
    "  model.eval() # Turn on the evaluation mode\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for i in range( 0, eval_data.size(0) -1, bptt ):\n",
    "      data, targets = get_batch( eval_data, i )\n",
    "      seq_len = data.size( 0 )\n",
    "      output = model( data )\n",
    "      output_flat = output.view( -1, ntokens )\n",
    "      total_loss += seq_len * criterion( output_flat, targets ).item()\n",
    "  return total_loss / ( len( eval_data ) - 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch  7.93 | loss  6.76 | ppl   859.52\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch  7.75 | loss  6.53 | ppl   688.61\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch  7.73 | loss  6.27 | ppl   527.78\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch  7.74 | loss  6.20 | ppl   493.28\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch  7.71 | loss  6.11 | ppl   451.24\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch  7.76 | loss  6.10 | ppl   445.94\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch  7.72 | loss  6.07 | ppl   433.94\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch  7.67 | loss  6.07 | ppl   431.96\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch  7.61 | loss  5.99 | ppl   399.64\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch  7.67 | loss  5.99 | ppl   400.09\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch  7.76 | loss  5.87 | ppl   355.75\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch  7.76 | loss  5.95 | ppl   382.64\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch  7.97 | loss  5.93 | ppl   377.12\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  7.92 | loss  5.86 | ppl   350.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 23.76s | valid loss  5.77 | valid ppl   321.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch  7.77 | loss  5.78 | ppl   324.42\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch  7.81 | loss  5.80 | ppl   331.07\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch  7.81 | loss  5.63 | ppl   279.25\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch  7.76 | loss  5.68 | ppl   291.92\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch  7.77 | loss  5.64 | ppl   280.08\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch  7.76 | loss  5.66 | ppl   288.15\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch  7.79 | loss  5.68 | ppl   291.56\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch  8.00 | loss  5.70 | ppl   298.97\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch  7.85 | loss  5.66 | ppl   285.79\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch  7.90 | loss  5.67 | ppl   288.63\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch  8.13 | loss  5.55 | ppl   257.03\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch  8.23 | loss  5.64 | ppl   282.83\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch  7.88 | loss  5.64 | ppl   281.22\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch  7.80 | loss  5.57 | ppl   263.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 24.05s | valid loss  5.64 | valid ppl   280.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch  7.80 | loss  5.54 | ppl   255.63\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch  7.77 | loss  5.59 | ppl   267.75\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch  7.73 | loss  5.40 | ppl   221.86\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch  7.89 | loss  5.48 | ppl   239.15\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch  7.77 | loss  5.43 | ppl   229.17\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch  7.80 | loss  5.47 | ppl   238.51\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch  7.83 | loss  5.49 | ppl   242.81\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch  7.81 | loss  5.52 | ppl   249.45\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch  7.80 | loss  5.48 | ppl   239.15\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch  8.14 | loss  5.49 | ppl   242.14\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch  7.91 | loss  5.36 | ppl   212.13\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch  7.93 | loss  5.46 | ppl   235.75\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch  7.97 | loss  5.48 | ppl   238.67\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch  7.95 | loss  5.41 | ppl   224.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 24.02s | valid loss  5.55 | valid ppl   257.53\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/tmpz7bnhj2d/best_model_params.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m       torch\u001b[38;5;241m.\u001b[39msave( model\u001b[38;5;241m.\u001b[39mstate_dict(), best_model_params_path )\n\u001b[1;32m     22\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict( \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_params_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m )\n",
      "File \u001b[0;32m~/.virtual_envs/TransformersML/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.virtual_envs/TransformersML/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.virtual_envs/TransformersML/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/tmpz7bnhj2d/best_model_params.pt'"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "  best_model_params_path = os.path.join( tempdir, 'best_model_params.pt' )\n",
    "\n",
    "  for epoch in range( 1, epochs + 1 ):\n",
    "    epoch_start_time = time.time()\n",
    "    train( model )\n",
    "    val_loss = evaluate( model, val_data )\n",
    "    val_ppl = math.exp( val_loss )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print( '-' * 89 )\n",
    "    print( f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}' )\n",
    "    print( '-' * 89 )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "      best_val_loss = val_loss\n",
    "      torch.save( model.state_dict(), best_model_params_path )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "model.load_state_dict( torch.load( best_model_params_path ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate( model, test_data )\n",
    "test_ppl = math.exp( test_loss )\n",
    "print( '=' * 89 )\n",
    "print( f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}' )\n",
    "print( '=' * 89 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
