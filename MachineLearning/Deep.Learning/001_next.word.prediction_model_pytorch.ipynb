{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Word Prediction Model & how to build it using now Pytorch\n",
    "1. We start collecting a diverse dataset of text documents.\n",
    "2. Preprocess the data by cleaning and tokenizing it.\n",
    "3. Prepare the data by creating input-output pairs.\n",
    "4. Engineer features such as word embeddings.\n",
    "5. Select an appropiate model like an LSTM or GPT.\n",
    "6. Train the model on the dataset while adjusting the hyperparameters.\n",
    "7. Improve the model by experimenting with different techniques and arquitectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open( '/home/xamanek/PythonProjects/ML_Transformers_001/Datasets/20240205b_sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding = 'utf-8' ) as file:\n",
    "  text = file.read()\n",
    "\n",
    "# Simple tokenizer function\n",
    "tokenizer = get_tokenizer( 'basic_english' )\n",
    "\n",
    "# Tokenize the text \n",
    "tokens = tokenizer( text )\n",
    "\n",
    "# Build vocabulary \n",
    "def yield_tokens( data_iter ):\n",
    "  for text in data_iter:\n",
    "    yield tokenizer( text )\n",
    "\n",
    "vocab = build_vocab_from_iterator( yield_tokens( [ text ] ), specials = [ \"<unk>\" ] )\n",
    "vocab.set_default_index( vocab[ \"<unk>\" ] )\n",
    "\n",
    "# Numericalize tokens \n",
    "numericalized_tokens = [ vocab[ token ] for token in tokens ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-gram sequences\n",
    "input_sequences = [] \n",
    "\n",
    "for line in text.split('\\n'):\n",
    "  token_list = [ vocab[ token ] for token in tokenizer( line ) ]\n",
    "  for i in range( 1, len( token_list ) ):\n",
    "    n_gram_sequence = token_list[ :i + 1 ]\n",
    "    input_sequences.append( torch.tensor( n_gram_sequence ) )\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max( len(seq) for seq in input_sequences )\n",
    "input_sequences_padded = pad_sequence( \n",
    "  input_sequences, \n",
    "  batch_first = True, \n",
    "  padding_value = 0 \n",
    ")\n",
    "\n",
    "# Create input and target sequences\n",
    "X = input_sequences_padded[ :, :-1 ]\n",
    "y = input_sequences_padded[ :, -1 ]\n",
    "\n",
    "# One-hot encode the target sequences\n",
    "y_one_hot = one_hot( y, num_classes = len( vocab ) ).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(8377, 100)\n",
      "  (lstm): LSTM(100, 1500, batch_first=True)\n",
      "  (fc): Linear(in_features=1500, out_features=8377, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel( nn.Module ):\n",
    "  def __init__( self, vocab_size, embedding_dim, lstm_units, output_size ):\n",
    "    super( LSTMModel, self ).__init__()\n",
    "    self.embedding = nn.Embedding( vocab_size, embedding_dim )\n",
    "    self.lstm = nn.LSTM( embedding_dim, lstm_units, batch_first = True )\n",
    "    self.fc = nn.Linear( lstm_units, output_size )\n",
    "\n",
    "  def forward( self, x ):\n",
    "    x = self.embedding( x )\n",
    "    _, (hidden, _) = self.lstm( x )\n",
    "    x = self.fc( hidden[ -1 ] )\n",
    "    return x\n",
    "  \n",
    "# Instantiate the model\n",
    "model = LSTMModel( len( vocab ), 100, 1500, len( vocab ) ).to( device )\n",
    "\n",
    "print( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset( Dataset ):\n",
    "  def __init__( self, sequences, targets ):\n",
    "    self.sequences = sequences\n",
    "    self.targets = targets\n",
    "\n",
    "  def __len__( self ):\n",
    "    return len( self.sequences )\n",
    "  \n",
    "  def __getitem__( self, idx ):\n",
    "    sequence = self.sequences[ idx ]\n",
    "    target = self.targets[ idx ]\n",
    "    return sequence, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset( X, y_one_hot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8196415215484346e-06, Perplexity: 1.2259835766290064\n",
      "Epoch 2, Loss: 1.6713863421536658e-06, Perplexity: 1.2058002535453238\n",
      "Epoch 3, Loss: 1.5965744295668518e-06, Perplexity: 1.1957419152338284\n",
      "Epoch 4, Loss: 1.8375160743032794e-06, Perplexity: 1.2284397120366486\n",
      "Epoch 5, Loss: 1.5455572064553106e-06, Perplexity: 1.18893089392893\n",
      "Epoch 6, Loss: 1.693082246037384e-06, Perplexity: 1.2087330271190317\n",
      "Epoch 7, Loss: 1.5331565658420624e-06, Perplexity: 1.1872812239858006\n",
      "Epoch 8, Loss: 1.5826129395763233e-06, Perplexity: 1.193874127108948\n",
      "Epoch 9, Loss: 1.4525674600070327e-06, Perplexity: 1.1766160047883603\n",
      "Epoch 10, Loss: 1.332992279677305e-06, Perplexity: 1.1609676199844143\n",
      "Epoch 11, Loss: 1.3156537992329124e-06, Perplexity: 1.1587159359718915\n",
      "Epoch 12, Loss: 8.842302124592415e-07, Perplexity: 1.104073335419174\n",
      "Epoch 13, Loss: 7.147976119582842e-07, Perplexity: 1.0833251716822168\n",
      "Epoch 14, Loss: 3.4676360313601398e-06, Perplexity: 1.4744244920838538\n",
      "Epoch 15, Loss: 1.4054835353558822e-06, Perplexity: 1.1704292782106858\n",
      "Epoch 16, Loss: 5.13686466971767e-07, Perplexity: 1.0592032346108236\n",
      "Epoch 17, Loss: 9.831919411624885e-07, Perplexity: 1.1163752114839895\n",
      "Epoch 18, Loss: 7.916382286555073e-07, Perplexity: 1.092686060766075\n",
      "Epoch 19, Loss: 1.0263468469491217e-06, Perplexity: 1.1217826032761398\n",
      "Epoch 20, Loss: 8.779039066482376e-07, Perplexity: 1.1032915418470717\n",
      "Epoch 21, Loss: 3.7980586548512827e-07, Perplexity: 1.0434436894774428\n",
      "Epoch 22, Loss: 7.13850231921926e-07, Perplexity: 1.0832102616811896\n",
      "Epoch 23, Loss: 3.3464731981630795e-07, Perplexity: 1.0381809817429617\n",
      "Epoch 24, Loss: 1.0436523423629555e-06, Perplexity: 1.123958365170336\n",
      "Epoch 25, Loss: 4.4301404848778744e-07, Perplexity: 1.0508547072971586\n",
      "Epoch 26, Loss: 9.266353691503926e-07, Perplexity: 1.1093280100520786\n",
      "Epoch 27, Loss: 8.666753016398929e-07, Perplexity: 1.1019052939124538\n",
      "Epoch 28, Loss: 3.04898926413088e-07, Perplexity: 1.034728659789409\n",
      "Epoch 29, Loss: 5.259661038690227e-07, Perplexity: 1.0606605757386025\n",
      "Epoch 30, Loss: 4.578182905545456e-07, Perplexity: 1.0525980655730336\n",
      "Epoch 31, Loss: 5.279477004372384e-07, Perplexity: 1.0608959384446015\n",
      "Epoch 32, Loss: 4.813182523808391e-07, Perplexity: 1.0553713794296677\n",
      "Epoch 33, Loss: 4.349799458516963e-07, Perplexity: 1.0499098145302967\n",
      "Epoch 34, Loss: 9.73112640228101e-07, Perplexity: 1.1151160154315718\n",
      "Epoch 35, Loss: 7.700652132448245e-07, Perplexity: 1.0900498529666451\n",
      "Epoch 36, Loss: 3.88651362001188e-07, Perplexity: 1.0444776503805588\n",
      "Epoch 37, Loss: 4.54841996835398e-07, Perplexity: 1.0522473429406993\n",
      "Epoch 38, Loss: 5.435542042110673e-07, Perplexity: 1.0627514160457578\n",
      "Epoch 39, Loss: 4.193113431789594e-07, Perplexity: 1.048069469910473\n",
      "Epoch 40, Loss: 4.807956513604127e-07, Perplexity: 1.0553096260602213\n",
      "Epoch 41, Loss: 4.951253340990881e-07, Perplexity: 1.0570042086078202\n",
      "Epoch 42, Loss: 9.473039033074139e-07, Perplexity: 1.1118982287695744\n",
      "Epoch 43, Loss: 7.678391352926725e-07, Perplexity: 1.0897781900211176\n",
      "Epoch 44, Loss: 6.815456346677443e-07, Perplexity: 1.079299244834506\n",
      "Epoch 45, Loss: 3.0157204089813244e-07, Perplexity: 1.0343432868217697\n",
      "Epoch 46, Loss: 5.944241234136775e-07, Perplexity: 1.0688219650001103\n",
      "Epoch 47, Loss: 3.7643861629928994e-07, Perplexity: 1.043050356639985\n",
      "Epoch 48, Loss: 5.719600165260333e-07, Perplexity: 1.0661369528562958\n",
      "Epoch 49, Loss: 8.358254836153679e-07, Perplexity: 1.098105633216276\n",
      "Epoch 50, Loss: 3.0221140736401884e-07, Perplexity: 1.0344173373054122\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam( model.parameters(), lr = 0.001 )\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "  total_loss = 0\n",
    "  for batch in DataLoader( dataset, batch_size = 128, shuffle = True ):\n",
    "    inputs, targets = batch\n",
    "\n",
    "    # Move to device\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    output = model( inputs )\n",
    "    loss = loss_function( output, targets )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "  print( f'Epoch {epoch + 1}, Loss: {total_loss / len(dataset)}, Perplexity: {np.exp( total_loss )}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initial text to generate next words\n",
    "seed_text = \"I don't know what to\"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range( next_words ):\n",
    "  # Tokenize the seed text\n",
    "  token_list = [ vocab[ token ] for token in tokenizer( seed_text ) ]\n",
    "\n",
    "  # cut the sequence if it is longer than the maximum sequence length\n",
    "  if len( token_list ) > max_sequence_len - 1:\n",
    "    token_list = token_list[ -(max_sequence_len - 1) : ]\n",
    "\n",
    "  # Pad the sequence\n",
    "  token_tensor = torch.tensor( [ token_list ] ).to( device )\n",
    "  token_tensor = pad_sequence( \n",
    "    token_tensor, \n",
    "    batch_first = True, \n",
    "    padding_value = 0\n",
    "  )\n",
    "\n",
    "  # Generate the next word\n",
    "  # No need to track the gradients\n",
    "  with torch.no_grad(): \n",
    "    # Get the index of the word with the highest probability\n",
    "    predicted = model( token_tensor ).argmax( dim = 1 ).item()\n",
    "\n",
    "  # find the word corresponding to the index \n",
    "  output_word = None \n",
    "  for word, index in vocab.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "\n",
    "  # if no word is found, terminate the loop\n",
    "  if output_word is None:\n",
    "    break\n",
    "\n",
    "\n",
    "  # Add the predicted word to the seed text\n",
    "  seed_text += ' ' + output_word\n",
    "\n",
    "print( seed_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what to do with ! ! ! ! ! ! ! ! ! !\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "unique_tokens = sorted(set(token for token in tokenizer(text)))\n",
    "stoi = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "itos = {idx: token for token, idx in stoi.items()}\n",
    "\n",
    "seed_text = \"I don't know what to do with\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range( next_words ):\n",
    "  # Tokenize the seed thext\n",
    "  token_list = [ \n",
    "    stoi[ token ] for token in tokenizer( seed_text ) \n",
    "    if token in stoi \n",
    "  ]\n",
    "\n",
    "  # Cut the sequence if it is longer than the maximum sequence length\n",
    "  if len( token_list ) > max_sequence_len -1: \n",
    "    token_list = token_list[ -( max_sequence_len - 1 ) : ]\n",
    "\n",
    "  # Pad the sequence\n",
    "  token_tensor = torch.tensor( [ token_list ] ).to( device )\n",
    "  token_tensor = pad_sequence( \n",
    "      token_tensor, \n",
    "      batch_first = True, \n",
    "      padding_value = 0 \n",
    "    )\n",
    "  \n",
    "  # Generate the next word, no need to track the gradients\n",
    "  with torch.no_grad():\n",
    "    # Get the index of the word with the highest probability\n",
    "    predicted = model( token_tensor ).argmax( dim = 1 ).item()\n",
    "\n",
    "  # use 'itos' to find the word corresponding to the index\n",
    "  output_word = itos[ predicted ] if predicted < len( itos ) else None\n",
    "\n",
    "  # if no word is found, terminate the loop\n",
    "  if output_word is None:\n",
    "    break\n",
    "\n",
    "  # Add the predicted word to the seed text\n",
    "  seed_text += ' ' + output_word\n",
    "\n",
    "print( seed_text )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
