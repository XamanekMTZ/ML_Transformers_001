{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Word Prediction Model & how to build it.\n",
    "\n",
    "In this notebook, we will build a next word prediction model, which means predicting the most\n",
    "likely word of phrase taht will come next in a sentence or text. It is like having an inbuilt \n",
    "feature on an application that suggests the next word as you type or speak. These models are \n",
    "used in many applications like messaging apps, search engines, virtual assistants, and \n",
    "autocorrect features on smartphones.\n",
    "\n",
    "NWP is a language modelling task in Machine Learning that aims to predict the most probable word \n",
    "or sequence of words that follows a given input context. This task uses statistical patterns and \n",
    "linguistic structures to generate accurate predictions based on the context provided.\n",
    "\n",
    "To build a NWP model:\n",
    "\n",
    "1. Start by collecting a diverse dataset of text documents.\n",
    "2. Preprocess the data by cleaning and tokenizing it.\n",
    "3. Preprae the data by creating input-output pairs.\n",
    "4. Engineer features such as word embeddings.\n",
    "5. Select an apporpiate model like and LSTM or GPT.\n",
    "6. Train the model on the dataset while adjusting hyperparameters.\n",
    "7. Improve the model by experimenting with different techniques and architectures.\n",
    "\n",
    "This iterative process allows businesses to develop accurate and efficient NWP models taht can be \n",
    "applied in diverse situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Read the text file \n",
    "# Change accordingly to the file path\n",
    "with open('/home/xamanek/PythonProjects/TransformersML/Datasets/20240205b_sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to create a sequence of words.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts( [ text ] )\n",
    "total_words = len( tokenizer.word_index ) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above tokenizes the text, which means it is divided into individual words or 'tokens'.  \n",
    "The 'Tokenizer' object is created, which will handle the tokenization process. The 'fit_on_texts'  \n",
    "method of the tokenizer is called, passing the 'text' as input. This method analyzes the text and   \n",
    "builds a vocabulary of unique words, assigning each word a numerical index. The 'total_words'  \n",
    "variable is then assigned the value of the length of the word index plus one, representing the   \n",
    "total number of distinct words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create input-output pairs by splitting the text into \n",
    "# sequences of tokens and forming n-grams from the sequences \n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "  token_list = tokenizer.texts_to_sequences( [ line ] )[ 0 ]\n",
    "  for i in range( 1, len( token_list ) ):\n",
    "    n_gram_sequence = token_list[ : i + 1 ]\n",
    "    input_sequences.append( n_gram_sequence )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above splits text data into lines using the '\\n' character as delimiter.  \n",
    "For each line in the text, the 'texts_to_sequences' method of the tokenizer is used  \n",
    "to convert the line into a sequence of numerical tokens based on the previously created  \n",
    "vocabulary. The resulting token list is then iterated over using a for loop. For each  \n",
    "iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning  \n",
    "of the token list up to the current index 'i'.  \n",
    "  \n",
    "This n-gram sequence represents the input context, with the last token being the target  \n",
    "predicted word. This n-gram sequence is then appended to the 'input_sequences' list.  \n",
    "This process is repeated for all lines in the text, generating multiple input-output  \n",
    "sequences that will be used for training the next word prediction model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets pad the input sequences to have equal length\n",
    "max_sequence_len = max( [ len(seq) for seq in input_sequences ] )\n",
    "input_sequences = np.array( pad_sequences( input_sequences, maxlen = max_sequence_len, padding = 'pre' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code creates the inpu sequences that are padded to ensure all sequences  \n",
    "have the same length. The variable 'max_sequence_len' is assigned the maximum length  \n",
    "among all the input sequences. The 'pad_sequences' function is used to pad or truncate  \n",
    "the input sequences to match this maximum length.  \n",
    "  \n",
    "The 'pad_sequences' function takes the input_sequences list, sets the maximum length to  \n",
    "'max_sequence_len', and specifies taht the padding should be added at the beginning of  \n",
    "each sequence using the 'padding = pre' argument. Finally, the input sequences are  \n",
    "converted into a numpy array to facilitate further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to split the sequences into input and output\n",
    "X = input_sequences[ :, :-1 ]\n",
    "y = input_sequences[ :, -1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences are split into two arrays, 'X' and 'y', to create the input  \n",
    "and output for training the next word prediction model. The 'X' array is assigned  \n",
    "the values of all rows in the 'input_sequences' array except for the last column.  \n",
    "It means that 'X' contains all the tokens in each sequence except for the last one,  \n",
    "representing the input context.  \n",
    "  \n",
    "On the other hand, the 'y' array is assigned the values of the last column in the  \n",
    "'input_sequences' array, which represents the target or predicted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output to one-hot encode vectors\n",
    "y = np.array(\n",
    "  tf.keras.utils.to_categorical(\n",
    "    y, num_classes = total_words\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2208800 (8.43 MB)\n",
      "Trainable params: 2208800 (8.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 20:18:44.520186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-05 20:18:44.520764: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# now to build a neural network architecture to train the model\n",
    "model = Sequential()\n",
    "model.add( \n",
    "  Embedding( \n",
    "    total_words, 100, input_length = max_sequence_len - 1 \n",
    "  ) \n",
    ")\n",
    "model.add(\n",
    "  LSTM(150)\n",
    ")\n",
    "model.add(\n",
    "  Dense(\n",
    "    total_words, activation = 'softmax'\n",
    "  )\n",
    ")\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines the model architeccture for the next word prediction model.  \n",
    "The 'Sequential' model is created, which represents a linear stack of layers.  \n",
    "First layer added to the model is the 'Embedding' layer, which is responsible for  \n",
    "converting the input sequences into dense vectors of fixed size. It takes 3 arguments:  \n",
    "  \n",
    "1. 'total_words', which represents the total number of distinct words in the vocabulary.\n",
    "2. '100', which denotes the dimensionality of the word embeddings.\n",
    "3. and 'input_length', which specifies the length of the input sequences. \n",
    "  \n",
    "The next layer added is the 'LSTM' (Long Short-Term Memory) layer, a type of recurrent neural network  \n",
    "(RNN) layer designed for capturing sequential dependencies in the data. It has 150 units,  \n",
    "which means it will learn 150 internal representations or memory cells.  \n",
    "  \n",
    "Finally, the 'Dense' layer is added, which is a fully connected layer that produces the output predictions.  \n",
    "It has 'total_words' units and uses the 'softmax' activation function to convert the predicted  \n",
    "scores into probabilities, indicating the likelihood of each word being the next one in  \n",
    "the sequences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 20:25:50.142743: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3159099200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010/3010 [==============================] - 65s 21ms/step - loss: 6.2601 - accuracy: 0.0753\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 5.5480 - accuracy: 0.1211\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 62s 20ms/step - loss: 5.1717 - accuracy: 0.1437\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 61s 20ms/step - loss: 4.8472 - accuracy: 0.1623\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 4.5439 - accuracy: 0.1800\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 61s 20ms/step - loss: 4.2598 - accuracy: 0.1989\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 3.9891 - accuracy: 0.2232\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 3.7293 - accuracy: 0.2507\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 3.4838 - accuracy: 0.2834\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 3.2558 - accuracy: 0.3171\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 3.0414 - accuracy: 0.3531\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.8440 - accuracy: 0.3869\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.6602 - accuracy: 0.4201\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.4932 - accuracy: 0.4529\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.3388 - accuracy: 0.4832\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.1971 - accuracy: 0.5135\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 2.0655 - accuracy: 0.5394\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 1.9475 - accuracy: 0.5650\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.8383 - accuracy: 0.5885\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 1.7375 - accuracy: 0.6112\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 1.6465 - accuracy: 0.6311\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.5600 - accuracy: 0.6478\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.4841 - accuracy: 0.6650\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.4124 - accuracy: 0.6798\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.3488 - accuracy: 0.6935\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.2873 - accuracy: 0.7074\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.2298 - accuracy: 0.7202\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 1.1802 - accuracy: 0.7311\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 1.1357 - accuracy: 0.7414\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 61s 20ms/step - loss: 1.0921 - accuracy: 0.7518\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.0525 - accuracy: 0.7601\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 1.0146 - accuracy: 0.7680\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.9785 - accuracy: 0.7754\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.9475 - accuracy: 0.7836\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.9186 - accuracy: 0.7898\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.8920 - accuracy: 0.7953\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.8671 - accuracy: 0.7996\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.8391 - accuracy: 0.8054\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.8202 - accuracy: 0.8101\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7984 - accuracy: 0.8157\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7805 - accuracy: 0.8172\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7629 - accuracy: 0.8210\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7461 - accuracy: 0.8256\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7311 - accuracy: 0.8287\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.7198 - accuracy: 0.8307\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.7049 - accuracy: 0.8328\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6941 - accuracy: 0.8359\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6802 - accuracy: 0.8385\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6666 - accuracy: 0.8424\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6622 - accuracy: 0.8409\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.6505 - accuracy: 0.8442\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6436 - accuracy: 0.8447\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6332 - accuracy: 0.8487\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6265 - accuracy: 0.8493\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6232 - accuracy: 0.8492\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.6127 - accuracy: 0.8514\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.6057 - accuracy: 0.8534\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.6019 - accuracy: 0.8534\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5975 - accuracy: 0.8539\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5919 - accuracy: 0.8546\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5852 - accuracy: 0.8564\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5820 - accuracy: 0.8566\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5788 - accuracy: 0.8570\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5697 - accuracy: 0.8592\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5654 - accuracy: 0.8609\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5702 - accuracy: 0.8572\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5601 - accuracy: 0.8610\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5560 - accuracy: 0.8617\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5546 - accuracy: 0.8619\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5551 - accuracy: 0.8612\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5469 - accuracy: 0.8633\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5478 - accuracy: 0.8625\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5404 - accuracy: 0.8650\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5446 - accuracy: 0.8624\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5401 - accuracy: 0.8638\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5376 - accuracy: 0.8636\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5375 - accuracy: 0.8631\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5355 - accuracy: 0.8647\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5275 - accuracy: 0.8666\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5297 - accuracy: 0.8657\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5266 - accuracy: 0.8653\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5285 - accuracy: 0.8645\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5235 - accuracy: 0.8650\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5201 - accuracy: 0.8663\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5253 - accuracy: 0.8653\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5175 - accuracy: 0.8670\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5225 - accuracy: 0.8649\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5136 - accuracy: 0.8676\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 61s 20ms/step - loss: 0.5189 - accuracy: 0.8660\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5135 - accuracy: 0.8674\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5103 - accuracy: 0.8676\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5113 - accuracy: 0.8683\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5121 - accuracy: 0.8668\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5087 - accuracy: 0.8671\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5120 - accuracy: 0.8667\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5081 - accuracy: 0.8675\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5132 - accuracy: 0.8654\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5057 - accuracy: 0.8675\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5083 - accuracy: 0.8665\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 60s 20ms/step - loss: 0.5084 - accuracy: 0.8673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a1693d9a0e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(\n",
    "  loss = 'categorical_crossentropy', \n",
    "  optimizer = 'adam', \n",
    "  metrics = [ 'accuracy' ]\n",
    ")\n",
    "model.fit(\n",
    "  X, y, epochs = 100, verbose = 1 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "I don't know what to say i have a fairly\n"
     ]
    }
   ],
   "source": [
    "# Now we can generate the next word predictions using our trained model\n",
    "seed_text = \"I don't know what to\"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range( next_words ):\n",
    "  token_list = tokenizer.texts_to_sequences( [ seed_text ] )[ 0 ]\n",
    "  token_list = pad_sequences( \n",
    "    [ token_list ], \n",
    "    maxlen = max_sequence_len - 1, \n",
    "    padding = 'pre' \n",
    "  )\n",
    "  predicted = np.argmax( model.predict( token_list ), axis = -1 )\n",
    "  output_word = \"\"\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "  seed_text += \" \" + output_word\n",
    "\n",
    "print( seed_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformersML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
